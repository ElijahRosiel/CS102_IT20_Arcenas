"title","author","subject","abstract","date"
"performance-aligned llms for generating fast code","daniel nichols, pranav polasam, harshitha menon, aniruddha marathe, todd gamblin, abhinav bhatele","distributed, parallel, and cluster computing","optimizing scientific software is a difficult task because codebases are often large and complex, and performance can depend upon several factors including the algorithm, its implementation, and hardware among others. causes of poor performance can originate from disparate sources and be difficult to diagnose. recent years have seen a multitude of work that use large language models (llms) to assist in software development tasks. however, these tools are trained to model the distribution of code as text, and are not specifically designed to understand performance aspects of code. in this work, we introduce a reinforcement learning based methodology to align the outputs of code llms with performance. this allows us to build upon the current code modeling capabilities of llms and extend them to generate better performing code. we demonstrate that our fine-tuned model improves the expected speedup of generated code over base models for a set of benchmark tasks from 0.9 to 1.6 for serial code and 1.9 to 4.5 for openmp code.",2024-04-29
"the dynamics of leadership and success in software development teams","lorenzo betti, luca gallo, johannes wachs, federico battiston","physics and society","from science to industry, teamwork plays a crucial role in knowledge production and innovation. most studies consider teams as static groups of individuals, thereby failing to capture how the micro-dynamics of collaborative processes and organizational changes determine team success. here, we leverage fine-grained temporal data on software development teams to gain insights into the dynamics of online collaborative projects. our analysis reveals an uneven workload distribution in teams, with stronger heterogeneity correlated with higher success, and the early emergence of a lead developer carrying out the majority of work. moreover, we find that a sizeable fraction of projects experience a change of lead developer, with such a transition being more likely in projects led by inexperienced users. finally, we show that leadership change is associated with faster success growth, in particular for the least successful projects. our work contributes to a deeper understanding of the link between team evolution and success in collaborative processes.",2024-04-29
"a framework to model ml engineering processes","sergio morales, robert clarisó, jordi cabot","software engineering","the development of machine learning (ml) based systems is complex and requires multidisciplinary teams with diverse skill sets. this may lead to communication issues or misapplication of best practices. process models can alleviate these challenges by standardizing task orchestration, providing a common language to facilitate communication, and nurturing a collaborative environment. unfortunately, current process modeling languages are not suitable for describing the development of such systems. in this paper, we introduce a framework for modeling ml-based software development processes, built around a domain-specific language and derived from an analysis of scientific and gray literature. a supporting toolkit is also available.",2024-04-29
"ai-powered code review with llms: early results","zeeshan rasheed, malik abdul sami, muhammad waseem, kai-kristian kemell, xiaofeng wang, anh nguyen, kari systä, pekka abrahamsson","software engineering","in this paper, we present a novel approach to improving software quality and efficiency through a large language model (llm)-based model designed to review code and identify potential issues. our proposed llm-based ai agent model is trained on large code repositories. this training includes code reviews, bug reports, and documentation of best practices. it aims to detect code smells, identify potential bugs, provide suggestions for improvement, and optimize the code. unlike traditional static code analysis tools, our llm-based ai agent has the ability to predict future potential risks in the code. this supports a dual goal of improving code quality and enhancing developer education by encouraging a deeper understanding of best practices and efficient coding techniques. furthermore, we explore the model's effectiveness in suggesting improvements that significantly reduce post-release bugs and enhance code review processes, as evidenced by an analysis of developer sentiment toward llm feedback. for future work, we aim to assess the accuracy and efficiency of llm-generated documentation updates in comparison to manual methods. this will involve an empirical study focusing on manually conducted code reviews to identify code smells and bugs, alongside an evaluation of best practice documentation, augmented by insights from developer discussions and code reviews. our goal is to not only refine the accuracy of our llm-based tool but also to underscore its potential in streamlining the software development lifecycle through proactive code improvement and education.",2024-04-29
"llmparser: an exploratory study on using large language models for log parsing","zeyang ma, an ran chen, dong jae kim, tse-hsun chen, shaowei wang","software engineering","logs are important in modern software development with runtime information. log parsing is the first step in many log-based analyses, that involve extracting structured information from unstructured log data. traditional log parsers face challenges in accurately parsing logs due to the diversity of log formats, which directly impacts the performance of downstream log-analysis tasks. in this paper, we explore the potential of using large language models (llms) for log parsing and propose llmparser, an llm-based log parser based on generative llms and few-shot tuning. we leverage four llms, flan-t5-small, flan-t5-base, llama-7b, and chatglm-6b in llmparsers. our evaluation of 16 open-source systems shows that llmparser achieves statistically significantly higher parsing accuracy than state-of-the-art parsers (a 96% average parsing accuracy). we further conduct a comprehensive empirical analysis on the effect of training size, model size, and pre-training llm on log parsing accuracy. we find that smaller llms may be more effective than more complex llms; for instance where flan-t5-base achieves comparable results as llama-7b with a shorter inference time. we also find that using llms pre-trained using logs from other systems does not always improve parsing accuracy. while using pre-trained flan-t5-base shows an improvement in accuracy, pre-trained llama results in a decrease (decrease by almost 55% in group accuracy). in short, our study provides empirical evidence for using llms for log parsing and highlights the limitations and future research direction of llm-based log parsers.",2024-04-27
"a survey of third-party library security research in application software","jia zeng, dan han, yaling zhu, yangzhong wang, fangchen weng","software engineering","in the current software development environment, third-party libraries play a crucial role. they provide developers with rich functionality and convenient solutions, speeding up the pace and efficiency of software development. however, with the widespread use of third-party libraries, associated security risks and potential vulnerabilities are increasingly apparent. malicious attackers can exploit these vulnerabilities to infiltrate systems, execute unauthorized operations, or steal sensitive information, posing a severe threat to software security. research on third-party libraries in software becomes paramount to address this growing security challenge. numerous research findings exist regarding third-party libraries' usage, ecosystem, detection, and fortification defenses. understanding the usage and ecosystem of third-party libraries helps developers comprehend the potential risks they bring and select trustworthy libraries. third-party library detection tools aid developers in automatically discovering third-party libraries in software, facilitating their management. in addition to detection, fortification defenses are also indispensable. this article profoundly investigates and analyzes this literature, summarizing current research achievements and future development directions. it aims to provide practical and valuable insights for developers and researchers, jointly promoting the healthy development of software ecosystems and better-protecting software from security threats.",2024-04-27
"using llms in software requirements specifications: an empirical evaluation","madhava krishna, bhagesh gaur, arsh verma, pankaj jalote","software engineering","the creation of a software requirements specification (srs) document is important for any software development project. given the recent prowess of large language models (llms) in answering natural language queries and generating sophisticated textual outputs, our study explores their capability to produce accurate, coherent, and structured drafts of these documents to accelerate the software development lifecycle. we assess the performance of gpt-4 and codellama in drafting an srs for a university club management system and compare it against human benchmarks using eight distinct criteria. our results suggest that llms can match the output quality of an entry-level software engineer to generate an srs, delivering complete and consistent drafts. we also evaluate the capabilities of llms to identify and rectify problems in a given requirements document. our experiments indicate that gpt-4 is capable of identifying issues and giving constructive feedback for rectifying them, while codellama's results for validation were not as encouraging. we repeated the generation exercise for four distinct use cases to study the time saved by employing llms for srs generation. the experiment demonstrates that llms may facilitate a significant reduction in development time for entry-level software engineers. hence, we conclude that the llms can be gainfully used by software engineers to increase productivity by saving time and effort in generating, validating and rectifying software requirements.",2024-04-27
"testing and understanding erroneous planning in llm agents through synthesized user inputs","zhenlan ji, daoyuan wu, pingchuan ma, zongjie li, shuai wang","artificial intelligence","agents based on large language models (llms) have demonstrated effectiveness in solving a wide range of tasks by integrating llms with key modules such as planning, memory, and tool usage. increasingly, customers are adopting llm agents across a variety of commercial applications critical to reliability, including support for mental well-being, chemical synthesis, and software development. nevertheless, our observations and daily use of llm agents indicate that they are prone to making erroneous plans, especially when the tasks are complex and require long-term planning.
in this paper, we propose pdoctor, a novel and automated approach to testing llm agents and understanding their erroneous planning. as the first work in this direction, we formulate the detection of erroneous planning as a constraint satisfiability problem: an llm agent's plan is considered erroneous if its execution violates the constraints derived from the user inputs. to this end, pdoctor first defines a domain-specific language (dsl) for user queries and synthesizes varying inputs with the assistance of the z3 constraint solver. these synthesized inputs are natural language paragraphs that specify the requirements for completing a series of tasks. then, pdoctor derives constraints from these requirements to form a testing oracle. we evaluate pdoctor with three mainstream agent frameworks and two powerful llms (gpt-3.5 and gpt-4). the results show that pdoctor can effectively detect diverse errors in agent planning and provide insights and error characteristics that are valuable to both agent developers and users. we conclude by discussing potential alternative designs and directions to extend pdoctor.",2024-04-27
"how do annotations affect java code readability?","eduardo guerra, everaldo gomes, jeferson ferreira, igor wiese, phyllipe lima, marco gerosa, paulo meirelles","software engineering","context: code annotations have gained widespread popularity in programming languages, offering developers the ability to attach metadata to code elements to define custom behaviors. many modern frameworks and apis use annotations to keep integration less verbose and located nearer to the corresponding code element. despite these advantages, practitioners' anecdotal evidence suggests that annotations might negatively affect code readability. objective: to better understand this effect, this paper systematically investigates the relationship between code annotations and code readability. method: in a survey with software developers (n=332), we present 15 pairs of java code snippets with and without code annotations. these pairs were designed considering five categories of annotation used in real-world java frameworks and apis. survey participants selected the code snippet they considered more readable for each pair and answered an open question about how annotations affect the code's readability. results: preferences were scattered for all categories of annotation usage, revealing no consensus among participants. the answers were spread even when segregated by participants' programming or annotation-related experience. nevertheless, some participants showed a consistent preference in favor or against annotations across all categories, which may indicate a personal preference. our qualitative analysis of the open-ended questions revealed that participants often praise annotation impacts on design, maintainability, and productivity but expressed contrasting views on understandability and code clarity. conclusions: software developers and api designers can consider our results when deciding whether to use annotations, equipped with the insight that developers express contrasting views of the annotations' impact on code readability.",2024-04-26
"unraveling code clone dynamics in deep learning frameworks","maram assi, safwat hassan, ying zou","software engineering","deep learning (dl) frameworks play a critical role in advancing artificial intelligence, and their rapid growth underscores the need for a comprehensive understanding of software quality and maintainability. dl frameworks, like other systems, are prone to code clones. code clones refer to identical or highly similar source code fragments within the same project or even across different projects. code cloning can have positive and negative implications for software development, influencing maintenance, readability, and bug propagation. in this paper, we aim to address the knowledge gap concerning the evolutionary dimension of code clones in dl frameworks and the extent of code reuse across these frameworks. we empirically analyze code clones in nine popular dl frameworks, i.e., tensorflow, paddle, pytorch, aesara, ray, mxnet, keras, jax and bentoml, to investigate (1) the characteristics of the long-term code cloning evolution over releases in each framework, (2) the short-term, i.e., within-release, code cloning patterns and their influence on the long-term trends, and (3) the file-level code clones within the dl frameworks. our findings reveal that dl frameworks adopt four distinct cloning trends and that these trends present some common and distinct characteristics. for instance, bug-fixing activities persistently happen in clones irrespective of the clone evolutionary trend but occur more in the ""serpentine"" trend. moreover, the within release level investigation demonstrates that short-term code cloning practices impact long-term cloning trends. the cross-framework code clone investigation reveals the presence of functional and architectural adaptation file-level cross-framework code clones across the nine studied frameworks. we provide insights that foster robust clone practices and collaborative maintenance in the development of dl frameworks.",2024-04-25
"what you use is what you get: unforced errors in studying cultural aspects in agile software development","michael neumann, klaus schmid, lars baumann","software engineering","context: cultural aspects are of high importance as they guide people's behaviour and thus, influence how people apply methods and act in projects. in recent years, software engineering research emphasized the need to analyze the challenges of specific cultural characteristics. investigating the influence of cultural characteristics is challenging due to the multi-faceted concept of culture. people's behaviour, their beliefs and underlying values are shaped by different layers of culture, e.g., regions, organizations, or groups. in this study, we focus on agile methods, which are agile approaches that focus on underlying values, collaboration and communication. thus, cultural and social aspects are of high importance for their successful use in practice. objective: in this paper, we address challenges that arise when using the model of cultural dimensions by hofstede to characterize specific cultural values. this model is often used when discussing cultural influences in software engineering. method: as a basis, we conducted an exploratory, multiple case study, consisting of two cases in japan and two in germany. contributions: in this study, we observed that cultural characteristics of the participants differed significantly from cultural characteristics that would typically be expected for people from the respective country. this drives our conclusion that for studies in empirical software engineering that address cultural factors, a case-specific analysis of the characteristics is needed.",2024-04-25
"a catalog of transformations to remove smells from natural language tests","manoel aranda, naelson oliveira, elvys soares, márcio ribeiro, davi romão, ullyanne patriota, rohit gheyi, emerson souza, ivan machado","software engineering","test smells can pose difficulties during testing activities, such as poor maintainability, non-deterministic behavior, and incomplete verification. existing research has extensively addressed test smells in automated software tests but little attention has been given to smells in natural language tests. while some research has identified and catalogued such smells, there is a lack of systematic approaches for their removal. consequently, there is also a lack of tools to automatically identify and remove natural language test smells. this paper introduces a catalog of transformations designed to remove seven natural language test smells and a companion tool implemented using natural language processing (nlp) techniques. our work aims to enhance the quality and reliability of natural language tests during software development. the research employs a two-fold empirical strategy to evaluate its contributions. first, a survey involving 15 software testing professionals assesses the acceptance and usefulness of the catalog's transformations. second, an empirical study evaluates our tool to remove natural language test smells by analyzing a sample of real-practice tests from the ubuntu os. the results indicate that software testing professionals find the transformations valuable. additionally, the automated tool demonstrates a good level of precision, as evidenced by a f-measure rate of 83.70%",2024-04-25
"legal aspects for software developers interested in generative ai applications","steffen herbold, brian valerius, anamaria mojica-hanke, isabella lex, joel mittel","software engineering","recent successes in generative artificial intelligence (genai) have led to new technologies capable of generating high-quality code, natural language, and images. the next step is to integrate genai technology into products, a task typically conducted by software developers. such product development always comes with a certain risk of liability. within this article, we want to shed light on the current state of two such risks: data protection and copyright. both aspects are crucial for genai. this technology deals with data for both model training and generated output. we summarize key aspects regarding our current knowledge that every software developer involved in product development using genai should be aware of to avoid critical mistakes that may expose them to liability claims.",2024-04-25
"fuzzy inference system for test case prioritization in software testing","aron karatayev, anna ogorodova, pakizar shamoi","software engineering","in the realm of software development, testing is crucial for ensuring software quality and adherence to requirements. however, it can be time-consuming and resource-intensive, especially when dealing with large and complex software systems. test case prioritization (tcp) is a vital strategy to enhance testing efficiency by identifying the most critical test cases for early execution. this paper introduces a novel fuzzy logic-based approach to automate tcp, using fuzzy linguistic variables and expert-derived fuzzy rules to establish a link between test case characteristics and their prioritization. our methodology utilizes two fuzzy variables - failure rate and execution time - alongside two crisp parameters: prerequisite test case and recently updated flag. our findings demonstrate the proposed system capacity to rank test cases effectively through experimental validation on a real-world software system. the results affirm the practical applicability of our approach in optimizing the tcp and reducing the resource intensity of software testing.",2024-04-25
"exploring human-ai collaboration in agile: customised llm meeting assistants","beatriz cabrero-daniel, tomas herda, victoria pichler, martin eder","software engineering","this action research study focuses on the integration of ""ai assistants"" in two agile software development meetings: the daily scrum and a feature refinement, a planning meeting that is part of an in-house scaled agile framework. we discuss the critical drivers of success, and establish a link between the use of ai and team collaboration dynamics. we conclude with a list of lessons learnt during the interventions in an industrial context, and provide a assessment checklist for companies and teams to reflect on their readiness level. this paper is thus a road-map to facilitate the integration of ai tools in agile setups.",2024-04-23
"open source software development tool installation: challenges and strategies for novice developers","larissa salerno, christoph treude, patanamon thongtatunam","software engineering","as the world of technology advances, so do the tools that software developers use to create new programs. in recent years, software development tools have become more popular, allowing developers to work more efficiently and produce higher-quality software. still, installing such tools can be challenging for novice developers at the early stage of their careers, as they may face challenges, such as compatibility issues (e.g., operating systems). therefore, this work aims to investigate the challenges novice developers face in software development when installing software development tools. to investigate these, we conducted an analysis of 24 live software installation sessions to observe challenges and comprehend their actions, the strategies they apply, and the type of source of information they consult when encountering challenges. our findings show that unclear documentation, such as installation instructions, and inadequate feedback during the installation process are common challenges faced by novice developers. moreover, reformulating search queries and relying on non-official documentation were some of the strategies employed to overcome challenges. based on our findings, we provide practical recommendations for tool vendors, tool users, and researchers.",2024-04-23
"llms in web-development: evaluating llm-generated php code unveiling vulnerabilities and limitations","rebeka tóth, tamas bisztray, lászló erdodi","software engineering","this research carries out a comprehensive examination of web application code security, when generated by large language models through analyzing a dataset comprising 2,500 small dynamic php websites. these ai-generated sites are scanned for security vulnerabilities after being deployed as standalone websites in docker containers. the evaluation of the websites was conducted using a hybrid methodology, incorporating the burp suite active scanner, static analysis, and manual checks. our investigation zeroes in on identifying and analyzing file upload, sql injection, stored xss, and reflected xss. this approach not only underscores the potential security flaws within ai-generated php code but also provides a critical perspective on the reliability and security implications of deploying such code in real-world scenarios. our evaluation confirms that 27% of the programs generated by gpt-4 verifiably contains vulnerabilities in the php code, where this number -- based on static scanning and manual verification -- is potentially much higher. this poses a substantial risks to software safety and security. in an effort to contribute to the research community and foster further analysis, we have made the source codes publicly available, alongside a record enumerating the detected vulnerabilities for each sample. this study not only sheds light on the security aspects of ai-generated code but also underscores the critical need for rigorous testing and evaluation of such technologies for software development.",2024-04-21
"assessing gpt-4-vision's capabilities in uml-based code generation","gábor antal, richárd vozár, rudolf ferenc","software engineering","the emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. this paper presents a preliminary evaluation of gpt-4-vision, a state-of-the-art deep learning model, and its capabilities in transforming unified modeling language (uml) class diagrams into fully operating java class files. in our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. we used 3 different prompts for each input, and we manually evaluated the results. we created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. on average, the model was able to generate source code for 88% of the elements shown in the diagrams. our results indicate that gpt-4-vision exhibits proficiency in handling single-class uml diagrams, successfully transforming them into syntactically correct class files. however, for multi-class uml diagrams, the model's performance is weaker compared to single-class diagrams. in summary, further investigations are necessary to exploit the model's potential completely.",2024-04-22
"llamp: assessing network latency tolerance of hpc applications with linear programming","siyuan shen, langwen huang, marcin chrapek, timo schneider, jai dayal, manisha gajbe, robert wisniewski, torsten hoefler","distributed, parallel, and cluster computing","the shift towards high-bandwidth networks driven by ai workloads in data centers and hpc clusters has unintentionally aggravated network latency, adversely affecting the performance of communication-intensive hpc applications. as large-scale mpi applications often exhibit significant differences in their network latency tolerance, it is crucial to accurately determine the extent of network latency an application can withstand without significant performance degradation. current approaches to assessing this metric often rely on specialized hardware or network simulators, which can be inflexible and time-consuming. in response, we introduce llamp, a novel toolchain that offers an efficient, analytical approach to evaluating hpc applications' network latency tolerance using the loggps model and linear programming. llamp equips software developers and network architects with essential insights for optimizing hpc infrastructures and strategically deploying applications to minimize latency impacts. through our validation on a variety of mpi applications like milc, lulesh, and lammps, we demonstrate our tool's high accuracy, with relative prediction errors generally below 2%. additionally, we include a case study of the icon weather and climate model to illustrate llamp's broad applicability in evaluating collective algorithms and network topologies.",2024-04-22
"teaching scrum with a focus on compliance assessment","marco torchiano, antonio vetrò, riccardo coppola","software engineering","the scrum framework has gained widespread adoption in the industry for its emphasis on collaboration and continuous improvement. however, it has not reached a similar relevance in software engineering (se) curricula. this work reports the experience of five editions of a se course within an msc. degree in computer engineering. the course primary educational objective is to provide students with the skills to manage software development projects with scrum. the course is based on the execution of a team project and on the definition of qualitative and quantitative means of assessment of the application of scrum. the conduction of five editions of the course allowed us to identify several lessons learned about time budgeting and team compositions in agile student projects and its evidence of the applicability of the framework to software development courses.",2024-04-22
"utilizing deep learning to optimize software development processes","keqin li, armando zhu, wenjing zhou, peng zhao, jintong song, jiabei liu","software engineering","this study explores the application of deep learning technologies in software development processes, particularly in automating code reviews, error prediction, and test generation to enhance code quality and development efficiency. through a series of empirical studies, experimental groups using deep learning tools and control groups using traditional methods were compared in terms of code error rates and project completion times. the results demonstrated significant improvements in the experimental group, validating the effectiveness of deep learning technologies. the research also discusses potential optimization points, methodologies, and technical challenges of deep learning in software development, as well as how to integrate these technologies into existing software development workflows.",2024-04-21
"paths to testing: why women enter and remain in software testing?","kleice silva, ann barcomb, ronnie de souza santos","software engineering","background. women bring unique problem-solving skills to software development, often favoring a holistic approach and attention to detail. in software testing, precision and attention to detail are essential as professionals explore system functionalities to identify defects. recognizing the alignment between these skills and women's strengths can derive strategies for enhancing diversity in software engineering. goal. this study investigates the motivations behind women choosing careers in software testing, aiming to provide insights into their reasons for entering and remaining in the field. method. this study used a cross-sectional survey methodology following established software engineering guidelines, collecting data from women in software testing to explore their motivations, experiences, and perspectives. findings. the findings reveal that women enter software testing due to increased entry-level job opportunities, work-life balance, and even fewer gender stereotypes. their motivations to stay include the impact of delivering high-quality software, continuous learning opportunities, and the challenges the activities bring to them. however, inclusiveness and career development in the field need improvement for sustained diversity. conclusion. preliminary yet significant, these findings offer interesting insights for researchers and practitioners towards the understanding of women's diverse motivations in software testing and how this understanding is important for fostering professional growth and creating a more inclusive and equitable industry landscape.",2024-04-20
"exploring hybrid work realities: a case study with software professionals from underrepresented groups","ronnie de souza santos, cleyton magalhes, robson santons, jorge correia-neto","software engineering","context. in the post-pandemic era, software professionals resist returning to office routines, favoring the flexibility gained from remote work. hybrid work structures, then, become popular within software companies, allowing them to choose not to work in the office every day, preserving flexibility, and creating several benefits, including an increase in the support for underrepresented groups in software development. goal. we investigated how software professionals from underrepresented groups are experiencing post-pandemic hybrid work. in particular, we analyzed the experiences of neurodivergents, lgbtqia+ individuals, and people with disabilities working in the software industry. method. we conducted a case study focusing on the underrepresented groups within a well-established south american software company. results. hybrid work is preferred by software professionals from underrepresented groups in the post-pandemic era. advantages include improved focus at home, personalized work setups, and accommodation for health treatments. concerns arise about isolation and inadequate infrastructure support, highlighting the need for proactive organizational strategies. conclusions. hybrid work emerges as a promising strategy for fostering diversity and inclusion in software engineering, addressing past limitations of the traditional office environment.",2024-04-20
"the visual debugger tool","tim kräuter, harald könig, adrian rutle, yngve lamo","software engineering","debugging is an essential part of software maintenance and evolution since it allows software developers to analyze program execution step by step. understanding a program is required to fix potential flaws, alleviate bottlenecks, and implement new desired features. thus, software developers spend a large percentage of their time validating and debugging software, resulting in high software maintenance and evolution cost. we aim to reduce this cost by providing a novel visual debugging tool to software developers to foster program comprehension during debugging. our debugging tool visualizes program execution information graphically as an object diagram and is fully integrated into the popular java development environment intellij idea. moreover, the object diagram allows interactions to explore program execution information in more detail. a demonstration of our tool is available at this https url.",2024-04-19
"a machine learning-based error mitigation approach for reliable software development on ibm's quantum computers","asmar muqeet, shaukat ali, tao yue, paolo arcaini","software engineering","quantum computers have the potential to outperform classical computers for some complex computational problems. however, current quantum computers (e.g., from ibm and google) have inherent noise that results in errors in the outputs of quantum software executing on the quantum computers, affecting the reliability of quantum software development. the industry is increasingly interested in machine learning (ml)--based error mitigation techniques, given their scalability and practicality. however, existing ml-based techniques have limitations, such as only targeting specific noise types or specific quantum circuits. this paper proposes a practical ml-based approach, called q-lear, with a novel feature set, to mitigate noise errors in quantum software outputs. we evaluated q-lear on eight quantum computers and their corresponding noisy simulators, all from ibm, and compared q-lear with a state-of-the-art ml-based approach taken as baseline. results show that, compared to the baseline, q-lear achieved a 25% average improvement in error mitigation on both real quantum computers and simulators. we also discuss the implications and practicality of q-lear, which, we believe, is valuable for practitioners.",2024-04-19
"how far are ai-powered programming assistants from meeting developers' needs?","xin tan, xiao long, xianjun ni, yinghao zhu, jing jiang, li zhang","software engineering","recent in-ide ai coding assistant tools (acats) like github copilot have significantly impacted developers' coding habits. while some studies have examined their effectiveness, there lacks in-depth investigation into the actual assistance process. to bridge this gap, we simulate real development scenarios encompassing three typical types of software development tasks and recruit 27 computer science students to investigate their behavior with three popular acats. our goal is to comprehensively assess acats' effectiveness, explore characteristics of recommended code, identify reasons for modifications, and understand users' challenges and expectations. to facilitate the study, we develop an experimental platform that includes a data collection plugin for vscode ide and provides functions for screen recording, code evaluation, and automatic generation of personalized interview and survey questions. through analysis of the collected data, we find that acats generally enhance task completion rates, reduce time, improve code quality, and increase self-perceived productivity. however, the improvement is influenced by both the nature of coding tasks and users' experience level. notably, for experienced participants, the use of acats may even increase completion time. we observe that ""edited line completion"" is the most frequently recommended way, while ""comments completion"" and ""string completion"" have the lowest acceptance rates. the primary reasons for modifying recommended code are disparities between output formats and requirements, flawed logic, and inconsistent code styles. in terms of challenges and expectations, optimization of service access and help documentation is also concerned by participants except for functionality and performance. our study provides valuable insights into the effectiveness and usability of acats, informing further improvements in their design and implementation.",2024-04-18
"from language models to practical self-improving computer agents","alex sheng","artificial intelligence","we develop a simple and straightforward methodology to create ai computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks. as large language models (llms) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments llms with various capabilities. rather than manually developing static software to augment llms through human engineering effort, we propose that an llm agent can systematically generate software to augment itself. we show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an llm to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks. starting with only terminal access, we prompt an llm agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities. the agent effectively uses these various tools to solve problems including automated software development and web-based tasks.",2024-04-18
"rethinking software engineering in the foundation model era: from task-driven ai copilots to goal-driven ai pair programmers","ahmed e. hassan, gustavo a. oliva, dayi lin, boyuan chen, zhen ming (jack)jiang","software engineering","the advent of foundation models (fms) and ai-powered copilots has transformed the landscape of software development, offering unprecedented code completion capabilities and enhancing developer productivity. however, the current task-driven nature of these copilots falls short in addressing the broader goals and complexities inherent in software engineering (se). in this paper, we propose a paradigm shift towards goal-driven ai-powered pair programmers that collaborate with human developers in a more holistic and context-aware manner. we envision ai pair programmers that are goal-driven, human partners, se-aware, and self-learning. these ai partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making. we discuss the desired attributes of such ai pair programmers and outline key challenges that must be addressed to realize this vision. ultimately, our work represents a shift from ai-augmented se to ai-transformed se by replacing code completion with a collaborative partnership between humans and ai that enhances both productivity and software quality.",2024-04-16
"hybrid work meets agile software development: a systematic mapping study","dron khanna, emily laue christensen, saagarika gosu, xiaofeng wang, maria paasivaara","software engineering","hybrid work, a fusion of different work environments that allow employees to work in and outside their offices, represents a new frontier for agile researchers to explore. however, due to the nascent nature of the research phenomena, we are yet to achieve a good understanding of the research terrain formulated when hybrid work meets agile software development. this systematic mapping study, we aimed to provide a good understanding of this emerging research area. the systematic process we followed led to a collection of 12 primary studies, which is less than what we expected. all the papers are empirical studies, with most of them employing case studies as the research methodology. the people-centric nature of agile methods is yet to be adequately reflected in the studies in this area. similarly, there is a lack of a richer understanding of hybrid work in terms of flexible work arrangements. our mapping study identified various research opportunities that can be explored in future research.",2024-04-15
"software development in the age of llms and xr","jesus m. gonzalez-barahona","software engineering","let's imagine that in a few years generative ai has changed software development dramatically, taking charge of most of the programming tasks. let's also assume that extended reality devices became ubiquitous, being the preferred interface for interacting with computers. this paper proposes how this situation would impact ides, by exploring how the development process would be affected, and analyzing which tools would be needed for supporting developers.",2024-04-15
"julia as a universal platform for statistical software development","david roodman","econometrics","like python and java, which are integrated into stata, julia is a free programming language that runs on all major operating systems. the julia package links stata to julia as well. users can transfer data between stata and julia at high speed, issue julia commands from stata to analyze and plot, and pass results back to stata. julia's econometric software ecosystem is not as mature as stata's or r's, or even python's. but julia is an excellent environment for developing high-performance numerical applications, which can then be called from many platforms. the boottest program for wild bootstrap-based inference (roodman et al. 2019) can call a julia back end for a 33-50% speed-up, even as the r package fwildclusterboot (fischer and roodman 2021) uses the same back end for inference after instrumental variables estimation. reghdfejl mimics reghdfe (correia 2016) in fitting linear models with high-dimensional fixed effects but calls an independently developed julia package for tenfold acceleration on hard problems. reghdfejl also supports nonlinear models--preliminarily, as the julia package for that purpose matures.",2024-04-14
"oss myths and facts","yukako iimura, masanari kondo, kazushi tomoto, yasutaka kamei, naoyasu ubayashi, shinobu saito","software engineering","we have selected six myths about the oss community and have tested whether they are true or not. the purpose of this report is to identify the lessons that can be learned from the development style of the oss community and the issues that need to be addressed in order to achieve better employee experience (ex) in software development within companies and organizations. the oss community has been led by a group of skilled developers known as hackers. we have great respect for the engineers and activities of the oss community and aim to learn from them. on the other hand, it is important to recognize that having high expectations can sometimes result in misunderstandings. when there are excessive expectations and concerns, misunderstandings (referred to as myths) can arise, particularly when individuals who are not practitioners rely on hearsay to understand the practices of practitioners. we selected the myths to be tested based on a literature review and interviews. these myths are held by software development managers and customers who are not direct participants in the oss community. we answered questions about each myth through: 1) our own analysis of repository data, 2) a literature survey of data analysis conducted by previous studies, or 3) a combination of the two approaches.",2024-04-14
"a large scale survey of motivation in software development and analysis of its validity","idan amit, dror g. feitelson","software engineering","context: motivation is known to improve performance. in software development in particular, there has been considerable interest in the motivation of contributors to open source. objective: we identify 11 motivators from the literature (enjoying programming, ownership of code, learning, self use, etc.), and evaluate their relative effect on motivation. since motivation is an internal subjective feeling, we also analyze the validity of the answers. method: we conducted a survey with 66 questions on motivation which was completed by 521 developers. most of the questions used an 11 point scale. we evaluated the validity of the answers validity by comparing related questions, comparing to actual behavior on github, and comparison with the same developer in a follow up survey. results: validity problems include moderate correlations between answers to related questions, as well as self promotion and mistakes in the answers. despite these problems, predictive analysis, investigating how diverse motivators influence the probability of high motivation, provided valuable insights. the correlations between the different motivators are low, implying their independence. high values in all 11 motivators predict increased probability of high motivation. in addition, improvement analysis shows that an increase in most motivators predicts an increase in general motivation.",2024-04-12
"linguaquanta: towards a quantum transpiler between openqasm and quipper (extended)","scott wesley","quantum physics","as quantum computing evolves, many important questions emerge, such as how best to represent quantum programs, and how to promote interoperability between quantum program analysis tools. these questions arise naturally in the design of quantum transpilers, which translate between quantum programming languages. in this paper, we take a step towards answering these questions by identifying challenges and best practices in quantum transpiler design. we base these recommendations on our experience designing linguaquanta, a quantum transpiler between quipper and openqasm. first, we provide categorical specifications for quantum transpilers, which aim to encapsulate the core principles of the unix philosophy. we then identify quantum circuit decompositions which we expect to be useful in quantum transpilation. with these foundations in place, we then discuss challenges faced during the implementation of linguaquanta, such as ancilla management and stability under round translation. to show that linguaquanta works in practice, a short tutorial is given for the example of quantum phase estimation. we conclude with recommendations for the future of linguaquanta, and for quantum software development tools more broadly.",2024-04-11
"devaic: a tool for security assessment of ai-generated code","domenico cotroneo, roberta de luca, pietro liguori","software engineering","context: ai code generators are revolutionizing code writing and software development, but their training on large datasets, including potentially untrusted source code, raises security concerns. furthermore, these generators can produce incomplete code snippets that are challenging to evaluate using current solutions. objective: this research work introduces devaic (detection of vulnerabilities in ai-generated code), a tool to evaluate the security of ai-generated python code, which overcomes the challenge of examining incomplete code. method: we followed a methodological approach that involved gathering vulnerable samples, extracting implementation patterns, and creating regular expressions to develop the proposed tool. the implementation of devaic includes a set of detection rules based on regular expressions that cover 35 common weakness enumerations (cwes) falling under the owasp top 10 vulnerability categories. results: we utilized four popular ai models to generate python code, which we then used as a foundation to evaluate the effectiveness of our tool. devaic demonstrated a statistically significant difference in its ability to detect security vulnerabilities compared to the state-of-the-art solutions, showing an f1 score and accuracy of 94% while maintaining a low computational cost of 0.14 seconds per code snippet, on average. conclusions: the proposed tool provides a lightweight and efficient solution for vulnerability detection even on incomplete code.",2024-04-11
"diversity's double-edged sword: analyzing race's effect on remote pair programming interactions","shandler a. mason, sandeep kaur kuttal","software engineering","remote pair programming is widely used in software development, but no research has examined how race affects these interactions. we embarked on this study due to the historical under representation of black developers in the tech industry, with white developers comprising the majority. our study involved 24 experienced developers, forming 12 gender-balanced same- and mixed-race pairs. pairs collaborated on a programming task using the think-aloud method, followed by individual retrospective interviews. our findings revealed elevated productivity scores for mixed-race pairs, with no differences in code quality between same- and mixed-race pairs. mixed-race pairs excelled in task distribution, shared decision-making, and role-exchange but encountered communication challenges, discomfort, and anxiety, shedding light on the complexity of diversity dynamics. our study emphasizes race's impact on remote pair programming and underscores the need for diverse tools and methods to address racial disparities for collaboration.",2024-04-11
"bridging gaps, building futures: advancing software developer diversity and inclusion through future-oriented research","sonja m. hyrynsalmi, sebastian baltes, chris brown, rafael prikladnicki, gema rodriguez-perez, alexander serebrenik, jocelyn simmonds, bianca trinkenreich, yi wang, grischa liebel","software engineering","software systems are responsible for nearly all aspects of modern life and society. however, the demographics of software development teams that are tasked with designing and maintaining these software systems rarely match the demographics of users. as the landscape of software engineering (se) evolves due to technological innovations, such as the rise of automated programming assistants powered by artificial intelligence (ai) and machine learning, more effort is needed to promote software developer diversity and inclusion (sddi) to ensure inclusive work environments for development teams and usable software for diverse populations. to this end, we present insights from se researchers and practitioners on challenges and solutions regarding diversity and inclusion in se. based on these findings, we share potential utopian and dystopian visions of the future and provide future research directions and implications for academia and industry to promote sddi in the age of ai-driven se.",2024-04-10
"on test sequence generation using multi-objective particle swarm optimization","zain iqbal, kashif zafar, aden iqbal, ayesha khan","software engineering","software testing is an important and essential part of the software development life cycle and accounts for almost one-third of system development costs. in the software industry, testing costs can account for about 35% to 40% of the total cost of a software project. therefore, providing efficient ways to test software is critical to reduce cost, time, and effort. black-box testing and white-box testing are two essential components of software testing. black-box testing focuses on the software's functionality, while white-box testing examines its internal structure. these tests contribute significantly to ensuring program coverage, which remains one of the main goals of the software testing paradigm. one of the main problems in this area is the identification of appropriate paths for program coverage, which are referred to as test sequences. creating an automated and effective test sequence is a challenging task in the software testing process. in the proposed methodology, the challenge of ""test sequence generation"" is considered a multi-objective optimization problem that includes the oracle cost and the path, both of which are optimized in a symmetrical manner to achieve optimal software testing. multi-objective particle swarm optimization (mopso) is used to represent the test sequences with the highest priority and the lowest oracle cost as optimal. the performance of the implemented approach is compared with the multi-objective firefly algorithm (mofa) for generating test sequences. the mopso-based solution outperforms the mofa-based approach and simultaneously provides the optimal solution for both objectives.",2024-04-09
"public-private funding models in open source software development: a case study on scikit-learn","cailean osborne","software engineering","governments are increasingly funding open source software (oss) development to support software security, digital sovereignty, and national competitiveness in science and innovation, amongst others. however, little is known about how oss developers evaluate the relative benefits and drawbacks of emergent governmental funding for oss. this paper explores this question through a case study on scikit-learn, a popular python library for machine learning, which has been funded by public research grants, commercial sponsorship, micro-donations, and a 32 million euro grant announced in france's artificial intelligence strategy. through 25 interviews with scikit-learn's maintainers and funders, this study makes two key contributions to research and practice. first, it contributes novel empirical findings on the effective design and implementation of a public-private funding model in an oss project, as well as how the maintainers of scikit-learn have designed and employed governance protocols to balance the diverse interests of their funders and to safeguard their community ethos. second, it offers practical lessons on funding in community-led oss projects and makes recommendations to practitioners. the paper concludes with a discussion of the key recommendations.",2024-04-09
"a rag method for source code inquiry tailored to long-context llms","toshihiro kamiya","software engineering","although the context length limitation of large language models (llms) has been mitigated, it still hinders their application to software development tasks. this study proposes a method incorporating execution traces into rag for inquiries about source code. small-scale experiments confirm a tendency for the method to contribute to improving llm response quality.",2024-04-09
"kamping: flexible and (near) zero-overhead c++ bindings for mpi","demian hespe, lukas hübner, florian kurpicz, peter sanders, matthias schimek, daniel seemaier, christoph stelz, tim niklas uhl","distributed, parallel, and cluster computing","the message-passing interface (mpi) and c++ form the backbone of high-performance computing, but mpi only provides c and fortran bindings. while this offers great language interoperability, high-level programming languages like c++ make software development quicker and less error-prone.
we propose novel c++ language bindings that cover all abstraction levels from low-level mpi calls to convenient stl-style bindings, where most parameters are inferred from a small subset of parameters, by bringing named parameters to c++. this enables rapid prototyping and fine-tuning runtime behavior and memory management. a flexible type system and additional safeness guarantees help to prevent programming errors.
by exploiting c++'s template-metaprogramming capabilities, this has (near) zero-overhead, as only required code paths are generated at compile time.
we demonstrate that our library is a strong foundation for a future distributed standard library using multiple application benchmarks, ranging from text-book sorting algorithms to phylogenetic interference.",2024-04-08
"synergy of large language model and model driven engineering for automated development of centralized vehicular systems","nenad petrovic, fengjunjie pan, krzysztof lebioda, vahid zolfaghari, sven kirchner, nils purschke, muhammad aqib khan, viktor vorobev, alois knoll","software engineering","we present a prototype of a tool leveraging the synergy of model driven engineering (mde) and large language models (llm) for the purpose of software development process automation in the automotive industry. in this approach, the user-provided input is free form textual requirements, which are first translated to ecore model instance representation using an llm, which is afterwards checked for consistency using object constraint language (ocl) rules. after successful consistency check, the model instance is fed as input to another llm for the purpose of code generation. the generated code is evaluated in a simulated environment using carla simulator connected to an example centralized vehicle architecture, in an emergency brake scenario.",2024-04-08
"autocoderover: autonomous program improvement","yuntong zhang, haifeng ruan, zhiyu fan, abhik roychoudhury","software engineering","researchers have made significant progress in automating the software development process in the past decades. recent progress in large language models (llms) has significantly impacted the development process, where developers can use llm-based programming assistants to achieve automated coding. nevertheless software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. bug fixing) and software evolution (e.g. feature additions). in this paper, we propose an automated approach for solving github issues to autonomously achieve program improvement. in our approach called autocoderover, llms are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. in contrast to recent llm agent approaches from ai researchers and practitioners, our outlook is more software engineering oriented. we work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. our code search exploits the program structure in the form of classes/methods to enhance llm's understanding of the issue's root cause, and effectively retrieve a context via iterative search. the use of spectrum based fault localization using tests, further sharpens the context, as long as a test-suite is available. experiments on swe-bench-lite which consists of 300 real-life github issues show increased efficacy in solving github issues (22-23% on swe-bench-lite). on the full swe-bench consisting of 2294 github issues, autocoderover solved around 16% of issues, which is higher than the efficacy of the recently reported ai software engineer devin from cognition labs, while taking time comparable to devin. we posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from llms can be autonomously improved.",2024-04-08
"ai for devsecops: a landscape and future opportunities","michael fu, jirat pasuksmit, chakkrit tantithamthavorn","software engineering","devops has emerged as one of the most rapidly evolving software development paradigms. with the growing concerns surrounding security in software systems, the devsecops paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the devops workflow. however, integrating security into the devops workflow can impact agility and impede delivery speed. recently, the advancement of artificial intelligence (ai) has revolutionized automation in various software domains, including software security. ai-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. they reduce manual efforts, which can be integrated into devops to ensure uninterrupted delivery speed and align with the devsecops paradigm simultaneously. this paper seeks to contribute to the critical intersection of ai and devsecops by presenting a comprehensive landscape of ai-driven security techniques applicable to devops and identifying avenues for enhancing security, trust, and efficiency in software development processes. we analyzed 99 research papers spanning from 2017 to 2023. specifically, we address two key research questions (rqs). in rq1, we identified 12 security tasks associated with the devops process and reviewed existing ai-driven security approaches. in rq2, we discovered 15 challenges encountered by existing ai-driven security approaches and derived future research opportunities. drawing insights from our findings, we discussed the state-of-the-art ai-driven security approaches, highlighted challenges in existing research, and proposed avenues for future opportunities.",2024-04-07
"search-based automated program repair of cps controllers modeled in simulink-stateflow","aitor arrieta, pablo valle, shaukat ali","software engineering","stateflow models are widely used in the industry to model the high-level control logic of cyber-physical systems (cpss) in simulink--the defacto cps simulator. many approaches exist to test simulink models, but once a fault is detected, the process to repair it remains manual. such a manual process increases the software development cost, making it paramount to develop novel techniques that reduce this cost. automated program repair (apr) techniques can significantly reduce the time for fixing bugs by automatically generating patches. however, current approaches face scalability issues to be applicable in the cps context. to deal with this problem, we propose an automated search-based approach called flowrepair, explicitly designed to repair stateflow models. the novelty of flowrepair includes, (1) a new algorithm that combines global and local search for patch generation; (2) a definition of novel repair objectives (e.g., the time a fault remained active) specifically designed for repairing cpss; and (3) a set of mutation operators to repair stateflow models automatically. we evaluated flowrepair with three different case study systems and a total of nine faulty stateflow models. our experiments suggest that (1) flo wrepaircan fix bugs in stateflow models, including models with multiple faults; (2) flowrepair surpasses or performs similarly to a baseline apr technique inspired by a well-known cps program repair approach. besides, we provide both a replication package and a live repository, paving the way towards the apr of cpss modeled in simulink.",2024-04-06
"towards understanding the impact of code modifications on software quality metrics","thomas karanikiotis, andreas l. symeonidis","software engineering","context: in the realm of software development, maintaining high software quality is a persistent challenge. however, this challenge is often impeded by the lack of comprehensive understanding of how specific code modifications influence quality metrics.
objective: this study ventures to bridge this gap through an approach that aspires to assess and interpret the impact of code modifications. the underlying hypothesis posits that code modifications inducing similar changes in software quality metrics can be grouped into distinct clusters, which can be effectively described using an ai language model, thus providing a simple understanding of code changes and their quality implications.
method: to validate this hypothesis, we built and analyzed a dataset from popular github repositories, segmented into individual code modifications. each project was evaluated against software quality metrics pre and post-application. machine learning techniques were utilized to cluster these modifications based on the induced changes in the metrics. simultaneously, an ai language model was employed to generate descriptions of each modification's function.
results: the results reveal distinct clusters of code modifications, each accompanied by a concise description, revealing their collective impact on software quality metrics.
conclusions: the findings suggest that this research is a significant step towards a comprehensive understanding of the complex relationship between code changes and software quality, which has the potential to transform software maintenance strategies and enable the development of more accurate quality prediction models.",2024-04-05
"x-lifecycle learning for cloud incident management using llms","drishti goel, fiza husain, aditya singh, supriyo ghosh, anjaly parayil, chetan bansal, xuchao zhang, saravan rajmohan","networking and internet architecture","incident management for large cloud services is a complex and tedious process and requires significant amount of manual efforts from on-call engineers (oces). oces typically leverage data from different stages of the software development lifecycle [sdlc] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root causing and mitigating of incidents. recent advancements in large language models [llms] (e.g., chatgpt, gpt-4, gemini) created opportunities to automatically generate contextual recommendations to the oces assisting them to quickly identify and mitigate critical issues. however, existing research typically takes a silo-ed view for solving a certain task in incident management by leveraging data from a single stage of sdlc. in this paper, we demonstrate that augmenting additional contextual data from different stages of sdlc improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying ontology of service monitors used for automatically detecting incidents. by leveraging 353 incident and 260 monitor dataset from microsoft, we demonstrate that augmenting contextual information from different stages of the sdlc improves the performance over state-of-the-art methods.",2024-02-15
"codeeditorbench: evaluating code editing capability of large language models","jiawei guo, ziming li, xueling liu, kaijing ma, tianyu zheng, zhouliang yu, ding pan, yizhi li, ruibo liu, yue wang, shuyue guo, xingwei qu, xiang yue, ge zhang, wenhu chen, jie fu","software engineering","large language models (llms) for code are rapidly evolving, with code editing emerging as a critical capability. we introduce codeeditorbench, an evaluation framework designed to rigorously assess the performance of llms in code editing tasks, including debugging, translating, polishing, and requirement switching. unlike existing benchmarks focusing solely on code generation, codeeditorbench emphasizes real-world scenarios and practical aspects of software development. we curate diverse coding challenges and scenarios from five sources, covering various programming languages, complexity levels, and editing tasks. evaluation of 19 llms reveals that closed-source models (particularly gemini-ultra and gpt-4), outperform open-source models in codeeditorbench, highlighting differences in model performance based on problem types and prompt sensitivities. codeeditorbench aims to catalyze advancements in llms by providing a robust platform for assessing code editing capabilities. we will release all prompts and datasets to enable the community to expand the dataset and benchmark emerging llms. by introducing codeeditorbench, we contribute to the advancement of llms in code editing and provide a valuable resource for researchers and practitioners.",2024-04-04
"enhancing student engagement in large-scale capstone courses: an experience report","asma shakil, paul denny","computers and society","computer science (cs) capstone courses offer students a valuable opportunity to gain hands-on experience in software development, practice essential soft skills, and enhance their employability prospects. they are a core component in many cs undergraduate degrees and address the acm curricula requirements of inculcating professional dispositions in students and making them aware of the broader societal implications of computing. however, coordinating a capstone course, especially for a large student cohort, can be a daunting task for academic staff. it demands considerable time and energy for planning and coordinating activities between students, academic staff, and any external stakeholders. in this experience report, we outline the iterative development and refinement of our capstone course as it grew substantially in size over a span of six consecutive sessions. we outline the pedagogies that helped us to enhance student engagement and motivation in the course as assessed by end-of-course surveys and students' written reflections. we share the lessons that we have learnt and provide recommendations to educators who are designing new capstone courses or looking to scale existing ones.",2024-04-03
"self-organized agents: a llm multi-agent framework toward ultra large-scale code generation and optimization","yoichi ishibashi, yoshimasa nishimura","software engineering","recent advancements in automatic code generation using large language model (llm) agent have brought us closer to the future of automated software development. however, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. to tackle this challenge, we propose self-organized multi-agent framework (soa), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. in soa, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. a key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. this enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. we evaluate soa on the humaneval benchmark and demonstrate that, compared to a single-agent system, each agent in soa handles significantly less code, yet the overall generated code is substantially greater. moreover, soa surpasses the powerful single-agent baseline by 5% in terms of pass@1 accuracy.",2024-04-02
"an exploratory study of the relationship between satd and other software development activities","shima esfandiari, ashkan sami","software engineering","technical debt is a common issue that arises when short-term gains are prioritized over long-term costs, leading to a degradation in the quality of the code. self-admitted technical debt (satd) is a specific type of technical debt that involves documenting code to remind developers of its debt. previous research has explored various aspects of satd, including detection methods, distribution, and its impact on software quality. to better understand satd, one comprehension technique is to examine its co-occurrence with other activities, such as refactoring and bug fixing. this study investigates the relationship between removing and adding satd and activities such as refactoring, bug fixing, adding new features, and testing. to do so, we analyzed 77 open-source java projects using todo/fixme/xxx removal or addition in inline comments as indicators of satd. we examined the co-occurrence of satd with each activity in each project through chi-square and odds ratio evaluations. our results show that satd removal occurs simultaneously with refactoring in 95% of projects, while its addition occurs in 89% of projects. furthermore, we found that three types of refactoring - ""move class"", ""remove method"", and ""move attribute"" - occur more frequently in the presence of satd. however, their distribution is similar in projects with and without satd.",2024-04-02
"automated user story generation with test case specification using large language model","tajmilur rahman, yuecai zhu","software engineering","modern software engineering era is moving fast with the assistance of artificial intelligence (ai), especially large language models (llm). researchers have already started automating many parts of the software development workflow. requirements engineering (re) is a crucial phase that begins the software development cycle through multiple discussions on a proposed scope of work documented in different forms. re phase ends with a list of user-stories for each unit task identified through discussions and usually these are created and tracked on a project management tool such as jira, azurdev etc. in this research we developed a tool ""geneus"" using gpt-4.0 to automatically create user stories from requirements document which is the outcome of the re phase. the output is provided in json format leaving the possibilities open for downstream integration to the popular project management tools. analyzing requirements documents takes significant effort and multiple meetings with stakeholders. we believe, automating this process will certainly reduce additional load off the software engineers, and increase the productivity since they will be able to utilize their time on other prioritized tasks.",2024-04-02
"octopus: on-device language model for function calling of software apis","wei chen, zhiyuan li, mingyuan ma","computation and language","in the rapidly evolving domain of artificial intelligence, large language models (llms) play a crucial role due to their advanced text processing and generation abilities. this study introduces a new strategy aimed at harnessing on-device llms in invoking software apis. we meticulously compile a dataset derived from software api documentation and apply fine-tuning to llms with capacities of 2b, 3b and 7b parameters, specifically to enhance their proficiency in software api interactions. our approach concentrates on refining the models' grasp of api structures and syntax, significantly enhancing the accuracy of api function calls. additionally, we propose \textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. we also propose a novel benchmark designed to evaluate the effectiveness of llms in api interactions, establishing a foundation for subsequent research. octopus, the fine-tuned model, is proved to have better performance than gpt-4 for the software apis calling. this research aims to advance automated software development and api integration, representing substantial progress in aligning llm capabilities with the demands of practical software engineering applications.",2024-04-02
"evaluating privacy perceptions, experience, and behavior of software development teams","maxwell prybylo, sara haghighi, sai teja peddinti, sepideh ghanavati","software engineering","with the increase in the number of privacy regulations, small development teams are forced to make privacy decisions on their own. in this paper, we conduct a mixed-method survey study, including statistical and qualitative analysis, to evaluate the privacy perceptions, practices, and knowledge of members involved in various phases of software development (sdlc). our survey includes 362 participants from 23 countries, encompassing roles such as product managers, developers, and testers. our results show diverse definitions of privacy across sdlc roles, emphasizing the need for a holistic privacy approach throughout sdlc. we find that software teams, regardless of their region, are less familiar with privacy concepts (such as anonymization), relying on self-teaching and forums. most participants are more familiar with gdpr and hipaa than other regulations, with multi-jurisdictional compliance being their primary concern. our results advocate the need for role-dependent solutions to address the privacy challenges, and we highlight research directions and educational takeaways to help improve privacy-aware software development.",2024-04-01
"peros: personalized self-adapting operating systems in the cloud","hongyu hè","human-computer interaction","operating systems (oses) are foundational to computer systems, managing hardware resources and ensuring secure environments for diverse applications. however, despite their enduring importance, the fundamental design objectives of oses have seen minimal evolution over decades. traditionally prioritizing aspects like speed, memory efficiency, security, and scalability, these objectives often overlook the crucial aspect of intelligence as well as personalized user experience. the lack of intelligence becomes increasingly critical amid technological revolutions, such as the remarkable advancements in machine learning (ml).
today's personal devices, evolving into intimate companions for users, pose unique challenges for traditional oses like linux and ios, especially with the emergence of specialized hardware featuring heterogeneous components. furthermore, the rise of large language models (llms) in ml has introduced transformative capabilities, reshaping user interactions and software development paradigms.
while existing literature predominantly focuses on leveraging ml methods for system optimization or accelerating ml workloads, there is a significant gap in addressing personalized user experiences at the os level. to tackle this challenge, this work proposes peros, a personalized os ingrained with llm capabilities. peros aims to provide tailored user experiences while safeguarding privacy and personal data through declarative interfaces, self-adaptive kernels, and secure data management in a scalable cloud-centric architecture; therein lies the main research question of this work: how can we develop intelligent, secure, and scalable oses that deliver personalized experiences to thousands of users?",2024-03-26
"an ultra-high-speed reproducing kernel particle method","siavash jafarzadeh, michael hillman","numerical analysis","in this work, the fast-convolving reproducing kernel particle method (fc-rkpm) is introduced. this method is hundreds to millions of times faster than the traditional rkpm for 3d meshfree simulations. in this approach, the meshfree discretizations with rk approximation are expressed in terms of convolution sums. fast fourier transform (fft) is then used to efficiently compute the convolutions. certain modifications to the domain and shape functions are considered to maintain generality for complex geometries and arbitrary boundary conditions. the new method does not need to identify, store, and loop over the neighbors which is one of the bottleneck of the traditional meshfree methods. as a result, the run-times and memory allocations are independent of the number of neighbors and the shape function support size. as a model problem, the method is laid out for a galerkin weak form of the poisson problem with the rk approximation, and is verified in 1d, 2d, and 3d. tables with run-times and allocated memory are presented to compare the performance of fc-rkpm with the traditional method in 3d. the performance is studied for various node numbers, support size, and approximation degree. all the implementation details and the roadmap for software development are also provided. application of the new method to nonlinear and explicit problems are briefly discussed as well.",2024-03-28
"cycling on the freeway: the perilous state of open source neuroscience software","britta u. westner, daniel r. mccloy, eric larson, alexandre gramfort, daniel s. katz, arfon m. smith, invited co-signees","computers and society","most scientists need software to perform their research (barker et al., 2020; carver et al., 2022; hettrick, 2014; hettrick et al., 2014; switters and osimo, 2019), and neuroscientists are no exception. whether we work with reaction times, electrophysiological signals, or magnetic resonance imaging data, we rely on software to acquire, analyze, and statistically evaluate the raw data we obtain - or to generate such data if we work with simulations. in recent years there has been a shift toward relying on free, open-source scientific software (fosss) for neuroscience data analysis (poldrack et al., 2019), in line with the broader open science movement in academia (mckiernan et al., 2016) and wider industry trends (eghbal, 2016). importantly, fosss is typically developed by working scientists (not professional software developers) which sets up a precarious situation given the nature of the typical academic workplace (wherein academics, especially in their early careers, are on short and fixed term contracts). in this paper, we will argue that the existing ecosystem of neuroscientific open source software is brittle, and discuss why and how the neuroscience community needs to come together to ensure a healthy growth of our software landscape to the benefit of all.",2024-03-28
"coderujb: an executable and unified java benchmark for practical programming scenarios","zhengran zeng, yidong wang, rui xie, wei ye, shikun zhang","software engineering","in the evolving landscape of large language models (llms) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. to address this, we introduce coderujb, a new benchmark designed to evaluate llms across diverse java programming tasks that are executable and reflective of actual development scenarios, acknowledging java's prevalence in real-world software production. coderujb comprises 2,239 programming questions derived from 17 real open-source java projects and spans five practical programming tasks. our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source llms, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. the findings indicate that while llms exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. coderujb thus marks a significant step towards more realistic evaluations of programming capabilities in llms, and our study provides valuable insights for the future development of these models in software engineering.",2024-03-28
"efficient multi-band temporal video filter for reducing human-robot interaction","lawrence o'gorman","robotics","although mobile robots have on-board sensors to perform navigation, their efficiency in completing paths can be enhanced by planning to avoid human interaction. infrastructure cameras can capture human activity continuously for the purpose of compiling activity analytics to choose efficient times and routes. we describe a cascade temporal filtering method to efficiently extract short- and long-term activity in two time dimensions, isochronal and chronological, for use in global path planning and local navigation respectively. the temporal filter has application either independently, or, if object recognition is also required, it can be used as a pre-filter to perform activity-gating of the more computationally expensive neural network processing. for a testbed 32-camera network, we show how this hybrid approach can achieve over 8 times improvement in frames per second throughput and 6.5 times reduction of system power use. we also show how the cost map of static objects in the ros robot software development framework is augmented with dynamic regions determined from the temporal filter.",2024-03-26
"building an open-source community to enhance autonomic nervous system signal analysis: dbdp-autonomic","jessilyn dunn, varun mishra, md mobashir hasan shandhi, hayoung jeong, natasha yamane, yuna watanabe, bill chen, matthew s. goodwin","human-computer interaction","smartphones and wearable sensors offer an unprecedented ability to collect peripheral psychophysiological signals across diverse timescales, settings, populations, and modalities. however, open-source software development has yet to keep pace with rapid advancements in hardware technology and availability, creating an analytical barrier that limits the scientific usefulness of acquired data. we propose a community-driven, open-source peripheral psychophysiological signal pre-processing and analysis software framework that could advance biobehavioral health by enabling more robust, transparent, and reproducible inferences involving autonomic nervous system data.",2024-03-25
"seeking enlightenment: incorporating evidence-based practice techniques in a research software engineering team","reed milewicz, jon bisila, miranda mundt, joshua teves","software engineering","evidence-based practice (ebp) in software engineering aims to improve decision-making in software development by complementing practitioners' professional judgment with high-quality evidence from research. we believe the use of ebp techniques may be helpful for research software engineers (rses) in their work to bring software engineering best practices to scientific software development. in this study, we present an experience report on the use of a particular ebp technique, rapid reviews, within an rse team at sandia national laboratories, and present practical recommendations for how to address barriers to ebp adoption within the rse community.",2024-03-25
"enhancing software effort estimation through reinforcement learning-based project management-oriented feature selection","haoyang chen, botong xu, kaiyang zhong","software engineering","purpose: the study aims to investigate the application of the data element market in software project management, focusing on improving effort estimation by addressing challenges faced by traditional methods. design/methodology/approach: this study proposes a solution based on feature selection, utilizing the data element market and reinforcement learning-based algorithms to enhance the accuracy of software effort estimation. it explores the application of the marlfs algorithm, customizing improvements to the algorithm and reward function. findings: this study demonstrates that the proposed approach achieves more precise estimation compared to traditional methods, leveraging feature selection to guide project management in software development. originality/value: this study contributes to the field by offering a novel approach that combines the data element market, machine learning, and feature selection to improve software effort estimation, addressing limitations of traditional methods and providing insights for future research in project management.",2024-03-25
"codes: natural language to code repository via multi-layer sketch","daoguang zan, ailun yu, wei liu, dong chen, bo shen, wei li, yafen yao, yongshun gong, xiaolin chen, bei guan, zhiguang yang, yongji wang, qianxiang wang, lizhen cui","computation and language","the impressive performance of large language models (llms) on code-related tasks has shown the potential of fully automated software development. in light of this, we introduce a new software engineering task, namely natural language to code repository (nl2repo). this task aims to generate an entire code repository from its natural language requirements. to address this task, we propose a simple yet effective framework codes, which decomposes nl2repo into multiple sub-tasks by a multi-layer sketch. specifically, codes includes three modules: reposketcher, filesketcher, and sketchfiller. reposketcher first generates a repository's directory structure for given requirements; filesketcher then generates a file sketch for each file in the generated structure; sketchfiller finally fills in the details for each function in the generated file sketch. to rigorously assess codes on the nl2repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. for benchmark-based evaluation, we craft a repository-oriented benchmark, sketcheval, and design an evaluation metric, sketchbleu. for feedback-based evaluation, we develop a vscode plugin for codes and engage 30 participants in conducting empirical studies. extensive experiments prove the effectiveness and practicality of codes on the nl2repo task.",2024-03-25
"a mixed method study of devops challenges","minaoar hossain tanzil, masud sarker, gias uddin, anindya iqbal","software engineering","context: devops practices combine software development and it operations. there is a growing number of devops related posts in popular online developer forum stack overflow (so). while previous research analyzed so posts related to build/release engineering, we are aware of no research that specifically focused on devops related discussions. objective: to learn the challenges developers face while using the currently available devops tools and techniques along with the organizational challenges in devops practices. method: we conduct an empirical study by applying topic modeling on 174k so posts that contain devops discussions. we then validate and extend the empirical study findings with a survey of 21 professional devops practitioners. results: we find that: (1) there are 23 devops topics grouped into four categories: cloud & ci/cd tools, infrastructure as code, container & orchestration, and quality assurance. (2) the topic category cloud & ci/cd tools contains the highest number of topics (10) which cover 48.6% of all questions in our dataset, followed by the category infrastructure as code (28.9%). (3) file management is the most popular topic followed by jenkins pipeline, while infrastructural exception handling and jenkins distributed architecture are the most difficult topics (with least accepted answers). (4) in the survey, developers mention that it requires hands-on experience before current devops tools can be considered easy. they raised the needs for better documentation and learning resources to learn the rapidly changing devops tools and techniques. practitioners also emphasized on the formal training approach by the organizations for devops skill development. conclusion: architects and managers can use the findings of this research to adopt appropriate devops technologies, and organizations can design tool or process specific devops training programs.",2024-03-25
"""how do people decide?"": a model for software library selection","minaoar hossain tanzil, gias uddin, ann barcomb","software engineering","modern-day software development is often facilitated by the reuse of third-party software libraries. despite the significant effort to understand the factors contributing to library selection, it is relatively unknown how the libraries are selected and what tools are still needed to support the selection process. using straussian grounded theory, we conducted and analyzed the interviews of 24 professionals across the world and derived a model of library selection process which is governed by six selection patterns (i.e., rules). the model draws from marketing theory and lays the groundwork for the development of a library selection tool which captures the technical and non-technical aspects developers consider.",2024-03-24
"explainable port mapping inference with sparse performance counters for amd's zen architectures","fabian ritter, sebastian hack","performance","performance models are instrumental for optimizing performance-sensitive code. when modeling the use of functional units of out-of-order x86-64 cpus, data availability varies by the manufacturer: instruction-to-port mappings for intel's processors are available, whereas information for amd's designs are lacking. the reason for this disparity is that standard techniques to infer exact port mappings require hardware performance counters that amd does not provide.
in this work, we modify the port mapping inference algorithm of the widely used this http url project to not rely on intel's performance counters. the modifications are based on a formal port mapping model with a counter-example-guided algorithm powered by an smt solver. we investigate in how far amd's processors comply with this model and where unexpected performance characteristics prevent an accurate port mapping. our results provide valuable insights for creators of cpu performance models as well as for software developers who want to achieve peak performance on recent amd cpus.",2024-03-24
"who uses personas in requirements engineering: the practitioners' perspective","yi wang, chetan arora, xiao liu, thuong hoang, vasudha malhotra, ben cheng, john grundy","software engineering","personas are commonly used in software projects to gain a better understanding of end-users' needs. however, there is a limited understanding of their usage and effectiveness in practice. this paper presents the results of a two-step investigation, comprising interviews with 26 software developers, ui/ux designers, business analysts and product managers and a survey of 203 practitioners, aimed at shedding light on the current practices, methods and challenges of using personas in software development. our findings reveal variations in the frequency and effectiveness of personas across different software projects and it companies, the challenges practitioners face when using personas and the reasons for not using them at all. furthermore, we investigate the coverage of human aspects in personas, often assumed to be a key feature of persona descriptions. contrary to the general perception, our study shows that human aspects are often ignored for various reasons in personas or requirements engineering in general. our study provides actionable insights for practitioners to overcome challenges in using personas during requirements engineering stages, and we identify areas for future research.",2024-03-23
"when llm-based code generation meets the software development process","feng lin, dong jae kim, tse-husn (peter)chen","software engineering","software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively. this paper introduces lcg, a code generation framework inspired by established software engineering practices. lcg leverages multiple large language model (llm) agents to emulate various software process models, namely lcgwaterfall, lcgtdd, and lcgscrum. each model assigns llm agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns. through collaborative efforts utilizing chain-of-thought and prompt composition techniques, the agents continuously refine themselves to enhance code quality. utilizing gpt3.5 as the underlying llm and baseline (gpt), we evaluate lcg across four code generation benchmarks: humaneval, humaneval-et, mbpp, and mbpp-et. results indicate lcgscrum outperforms other models, achieving pass@1 scores of 75.2, 65.5, 82.5, and 56.7 in humaneval, humaneval-et, mbpp, and mbpp-et, respectively - an average 15% improvement over gpt. analysis reveals distinct impacts of development activities on generated code, with design and code reviews contributing to enhanced exception handling, while design, testing, and code reviews mitigate code smells. furthermore, temperature values exhibit negligible influence on pass@1 across all models. however, variations in pass@1 are notable for different gpt3.5 model versions, ranging from 5 to over 60 in humaneval, highlighting the stability of lcg across model versions. this stability underscores the importance of adopting software process models to bolster the quality and consistency of llm-generated code.",2024-03-23
"just another copy and paste? comparing the security vulnerabilities of chatgpt generated code and stackoverflow answers","sivana hamer, marcelo d'amorim, laurie williams","software engineering","sonatype's 2023 report found that 97% of developers and security leads integrate generative artificial intelligence (ai), particularly large language models (llms), into their development process. concerns about the security implications of this trend have been raised. developers are now weighing the benefits and risks of llms against other relied-upon information sources, such as stackoverflow (so), requiring empirical data to inform their choice. in this work, our goal is to raise software developers awareness of the security implications when selecting code snippets by empirically comparing the vulnerabilities of chatgpt and stackoverflow. to achieve this, we used an existing java dataset from so with security-related questions and answers. then, we asked chatgpt the same so questions, gathering the generated code for comparison. after curating the dataset, we analyzed the number and types of common weakness enumeration (cwe) vulnerabilities of 108 snippets from each platform using codeql. chatgpt-generated code contained 248 vulnerabilities compared to the 302 vulnerabilities found in so snippets, producing 20% fewer vulnerabilities with a statistically significant difference. additionally, chatgpt generated 19 types of cwe, fewer than the 22 found in so. our findings suggest developers are under-educated on insecure code propagation from both platforms, as we found 274 unique vulnerabilities and 25 types of cwe. any code copied and pasted, created by ai or humans, cannot be trusted blindly, requiring good software engineering practices to reduce risk. future work can help minimize insecure code propagation from any platform.",2024-03-22
"towards deep learning enabled cybersecurity risk assessment for microservice architectures","majid abdulsatar, hussain ahmad, diksha goel, faheem ullah","software engineering","the widespread adoption of microservice architectures has given rise to a new set of software security challenges. these challenges stem from the unique features inherent in microservices. it is important to systematically assess and address software security challenges such as software security risk assessment. however, existing approaches prove inefficient in accurately evaluating the security risks associated with microservice architectures. to address this issue, we propose cyberwise predictor, a framework designed for predicting and assessing security risks associated with microservice architectures. our framework employs deep learning-based natural language processing models to analyze vulnerability descriptions for predicting vulnerability metrics to assess security risks. our experimental evaluation shows the effectiveness of cyberwise predictor, achieving an average accuracy of 92% in automatically predicting vulnerability metrics for new vulnerabilities. our framework and findings serve as a guide for software developers to identify and mitigate security risks in microservice architectures.",2024-03-22
"allhands: ask me anything on large-scale verbatim feedback via large language models","chaoyun zhang, zicheng ma, yuhao wu, shilin he, si qin, minghua ma, xiaoting qin, yu kang, yuyi liang, xiaoyu gou, yajie xue, qingwei lin, saravan rajmohan, dongmei zhang, qi zhang","software engineering","verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development. effectively and efficiently extracting valuable insights from such data poses a challenging task. this paper introduces allhands , an innovative analytic framework designed for large-scale feedback analysis through a natural language interface, leveraging large language models (llms). allhands adheres to a conventional feedback analytic workflow, initially conducting classification and topic modeling on the feedback to convert them into a structurally augmented format, incorporating llms to enhance accuracy, robustness, generalization, and user-friendliness. subsequently, an llm agent is employed to interpret users' diverse questions in natural language on feedback, translating them into python code for execution, and delivering comprehensive multi-modal responses, including text, code, tables, and images.
we evaluate allhands across three diverse feedback datasets. the experiments demonstrate that allhands achieves superior efficacy at all stages of analysis, including classification and topic modeling, eventually providing users with an ""ask me anything"" experience with comprehensive, correct and human-readable response. to the best of our knowledge, allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface.",2024-03-22
"programmers prefer individually assigned tasks vs. shared responsibility","adela krylova, roman makarov, sergei pasynkov, yegor bugayenko","software engineering","in traditional management, tasks are typically assigned to individuals, with each worker taking full responsibility for the success or failure of a task. in contrast, modern agile, lean, and extreme programming practices advocate for shared responsibility, where an entire group is accountable for the outcome of a project or task. despite numerous studies in other domains, the preferences of programmers have not been thoroughly analyzed. to address this gap, we conducted a survey featuring seven situational questions and collected the opinions of 120 software development practitioners. our findings reveal that programmers prefer tasks to be assigned to them on an individual basis and appreciate taking personal responsibility for failures, as well as receiving individual rewards for successes. understanding these preferences is crucial for project managers aiming to optimize team dynamics and ensure the successful completion of software projects.",2024-03-22
"multi-role consensus through llms discussions for vulnerability detection","zhenyu mao, jialong li, dongming jin, munan li, kenji tei","software engineering","recent advancements in large language models (llms) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. to this end, this paper introduces a multi-role approach to employ llms to act as different roles simulating a real-life code review process and engaging in discussions toward a consensus on the existence and classification of vulnerabilities in the code. preliminary evaluation of this approach indicates a 13.48% increase in the precision rate, an 18.25% increase in the recall rate, and a 16.13% increase in the f1 score.",2024-03-21
"pricing-driven development and operation of saas : challenges and opportunities","alejandro garcía-fernández, josé antonio parejo, antonio ruiz-cortés","software engineering","as the software as a service (saas) paradigm continues to reshape the software industry, a nuanced understanding of its operational dynamics becomes increasingly crucial. this paper delves into the intricate relationship between pricing strategies and software development within the saas model. using petclinic as a case study, we explore the implications of a pricing-driven development and operation approach of saas systems, highlighting the delicate balance between business-driven decision-making and technical implementation challenges, shedding light on how pricing plans can shape software features and deployment. our discussion aims to provide strategic insights for the community to navigate the complexities of this integrated approach, fostering a better alignment between business models and technological capabilities for effective cloud-based services.",2024-03-20
"quantitative analysis of ai-generated texts in academic research: a study of ai presence in arxiv submissions using ai detection tool","arslan akram","digital libraries","many people are interested in chatgpt since it has become a prominent aigc model that provides high-quality responses in various contexts, such as software development and maintenance. misuse of chatgpt might cause significant issues, particularly in public safety and education, despite its immense potential. the majority of researchers choose to publish their work on arxiv. the effectiveness and originality of future work depend on the ability to detect ai components in such contributions. to address this need, this study will analyze a method that can see purposely manufactured content that academic organizations use to post on arxiv. for this study, a dataset was created using physics, mathematics, and computer science articles. using the newly built dataset, the following step is to put this http url through its paces. the statistical analysis shows that this http url is very accurate, with a rate of 98%.",2024-02-09
"elevating software quality in agile environments: the role of testing professionals in unit testing","lucas neves, oscar campos, robson santos, italo santos, cleyton magalhaes, ronnie de souza santos","software engineering","testing is an essential quality activity in the software development process. usually, a software system is tested on several levels, starting with unit testing that checks the smallest parts of the code until acceptance testing, which is focused on the validations with the end-user. historically, unit testing has been the domain of developers, who are responsible for ensuring the accuracy of their code. however, in agile environments, testing professionals play an integral role in various quality improvement initiatives throughout each development cycle. this paper explores the participation of test engineers in unit testing within an industrial context, employing a survey-based research methodology. our findings demonstrate that testing professionals have the potential to strengthen unit testing by collaborating with developers to craft thorough test cases and fostering a culture of mutual learning and cooperation, ultimately contributing to increasing the overall quality of software projects.",2024-03-20
"python fuzzing for trustworthy machine learning frameworks","ilya yegorov, eli kobrin, darya parygina, alexey vishnyakov, andrey fedotov","cryptography and security","ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy ai-based systems. fuzzing, a popular technique in secure software development lifecycle (ssdlc), can be used to develop secure and robust software. popular machine learning frameworks such as pytorch and tensorflow are complex and written in multiple programming languages including c/c++ and python. we propose a dynamic analysis pipeline for python projects using the sydr-fuzz toolset. our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection. crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly. furthermore, the proposed pipeline is integrated in gitlab ci. to identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for pytorch, tensorflow, and related projects such as h5py. applying our dynamic analysis pipeline to these targets, we were able to discover 3 new bugs and propose fixes for them.",2024-03-19
"on the effectiveness of large language models for github workflows","xinyu zhang, siddharth muralee, sourag cherupattamoolayil, aravind machiry","software engineering","github workflows or github ci is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows, i.e., yaml files with a list of jobs. however, engineering valid workflows is tedious. they are also prone to severe security issues, which can result in supply chain vulnerabilities. recent advancements in large language models (llms) have demonstrated their effectiveness in various software development tasks. however, github workflows differ from regular programs in both structure and semantics. we perform the first comprehensive study to understand the effectiveness of llms on five workflow-related tasks with different levels of prompts. we curated a set of $\sim$400k workflows and generated prompts with varying detail. we also fine-tuned llms on github workflow tasks. our evaluation of three state-of-the-art llms and their fine-tuned variants revealed various interesting findings on the current effectiveness and drawbacks of llms.",2024-03-19
"advancing quantum software engineering: a vision of hybrid full-stack iterative model","arif ali khan, davide taibi, cécile m. perrault, asif ali khan","software engineering","this paper introduces a vision for quantum software development lifecycle, proposing a hybrid full-stack iterative model that integrates quantum and classical computing. addressing the current challenges in quantum computing (qc) such as the need for integrating diverse programming languages and managing the complexities of quantum-classical systems, this model is rooted in the principles of devops and continuous software engineering. it presents a comprehensive lifecycle for quantum software development, encompassing quantum-agnostic coding, testing, deployment, cloud computing services, orchestration, translation, execution, and interpretation phases. each phase is designed to accommodate the unique demands of qc, enabling traditional software developers to engage with qc environments without needing in-depth qc expertise. the paper presents a detailed implementation roadmap, utilizing a range of existing tools and frameworks, thereby making quantum software development more accessible and efficient. the proposed model not only addresses current challenges in quantum software development but also makes a substantial contribution to the field of quantum software engineering (qse). by proposing a structured and accessible model, it sets the stage for further advancements and research in qse, enhancing its practicality and relevance in a wide range of applications.",2024-03-18
"jaxdecompiler: redefining gradient-informed software design","pierrick pochelu","programming languages","among numerical libraries capable of computing gradient descent optimization, jax stands out by offering more features, accelerated by an intermediate representation known as jaxpr language. however, editing the jaxpr code is not directly possible. this article introduces jaxdecompiler, a tool that transforms any jax function into an editable python code, especially useful for editing the jax function generated by the gradient function. jaxdecompiler simplifies the processes of reverse engineering, understanding, customizing, and interoperability of software developed by jax. we highlight its capabilities, emphasize its practical applications especially in deep learning and more generally gradient-informed software, and demonstrate that the decompiled code speed performance is similar to the original.",2024-03-14
"an empirical study on developers shared conversations with chatgpt in github pull requests and issues","huizi hao, kazi amit hasan, hong qin, marcos macedo, yuan tian, steven h. h. ding, ahmed e. hassan","software engineering","chatgpt has significantly impacted software development practices, providing substantial assistance to developers in a variety of tasks, including coding, testing, and debugging. despite its widespread adoption, the impact of chatgpt as an assistant in collaborative coding remains largely unexplored. in this paper, we analyze a dataset of 210 and 370 developers shared conversations with chatgpt in github pull requests (prs) and issues. we manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them. our main observations are: (1) developers seek chatgpt assistance across 16 types of software engineering inquiries. in both conversations shared in prs and issues, the most frequently encountered inquiry categories include code generation, conceptual questions, how-to guides, issue resolution, and code review. (2) developers frequently engage with chatgpt via multi-turn conversations where each prompt can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and prompt refinement. multi-turn conversations account for 33.2% of the conversations shared in prs and 36.9% in issues. (3) in collaborative coding, developers leverage shared conversations with chatgpt to facilitate their role-specific contributions, whether as authors of prs or issues, code reviewers, or collaborators on issues. our work serves as the first step towards understanding the dynamics between developers and chatgpt in collaborative software development and opens up new directions for future research on the topic.",2024-03-15
"gamified gui testing with selenium in the intellij ide: a prototype plugin","giacomo garaccione, tommaso fulcini, paolo stefanut bodnarescul, riccardo coppola, luca ardito","software engineering","software testing is a crucial phase in software development, enabling the detection of issues and defects that may arise during the development process. addressing these issues enhances software applications' quality, reliability, user experience, and performance. graphical user interface (gui) testing, one such technique, involves mimicking a regular user's interactions with an application to identify defects. however, gui testing is often underutilized due to its perceived repetitiveness, error-proneness, and lack of immediate feedback on test quality. in recent years, gamification-incorporating game elements in non-game contexts to boost interest, motivation, and engagement-has gained traction in various fields, including software engineering and education. this paper presents gipgut: a prototype of a gamification plugin for intellij idea, an integrated development environment (ide) that supports scripted gui testing. the plugin enhances testers' engagement with typically monotonous and tedious tasks through achievements, rewards, and profile customization. a preliminary prototype evaluation was conducted with a small group of users to assess its usability and the impact of gamification on the gui testing process. the results indicate high usability and positive reception of the gamification elements. however, due to the limited sample size of participants, further research is necessary to understand the plugin's effectiveness fully.",2024-03-14
"how do machine learning projects use continuous integration practices? an empirical study on github actions","joão helis bernardo, daniel alencar da costa, sérgio queiroz de medeiros, uirá kulesza","software engineering","continuous integration (ci) is a well-established practice in traditional software development, but its nuances in the domain of machine learning (ml) projects remain relatively unexplored. given the distinctive nature of ml development, understanding how ci practices are adopted in this context is crucial for tailoring effective approaches. in this study, we conduct a comprehensive analysis of 185 open-source projects on github (93 ml and 92 non-ml projects). our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in ci adoption between ml and non-ml projects. our findings indicate that ml projects often require longer build durations, and medium-sized ml projects exhibit lower test coverage compared to non-ml projects. moreover, small and medium-sized ml projects show a higher prevalence of increasing build duration trends compared to their non-ml counterparts. additionally, our qualitative analysis illuminates the discussions around ci in both ml and non-ml projects, encompassing themes like ci build execution and status, ci testing, and ci infrastructure. these insights shed light on the unique challenges faced by ml projects in adopting ci practices effectively.",2024-03-14
"on stpa for distributed development of safe autonomous driving: an interview study","ali nouri, christian berger, fredrik törner","software engineering","safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions. this is especially true for complex ai-enabled and software intensive systems such as autonomous drive (ad). system-theoretic process analysis (stpa) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry. however, stpa assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels. this would inhibit software developers from using stpa to analyze their software as part of a bigger system, resulting in a lack of traceability. this can be seen as a maintainability challenge in continuous development and deployment (devops). in this paper, stpa's different guidelines for the automotive industry, e.g. j31887/iso21448/stpa handbook, are firstly compared to assess their applicability to the distributed development of complex ai-enabled systems like ad. further, an approach to overcome the challenges of using stpa in a multi-level design context is proposed. by conducting an interview study with automotive industry experts for the development of ad, the challenges are validated and the effectiveness of the proposed approach is evaluated.",2024-03-14
"code revert prediction with graph neural networks: a case study at j.p. morgan chase","yulong pei, salwa alamir, rares dolga, sameena shah","software engineering","code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. this task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. however, compared to code defect detection, code revert prediction has been rarely studied in previous research. additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. to overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features. different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection. we conduct the experiments on real-world code commit data within j.p. morgan chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.",2024-03-14
"an industrial experience report about challenges from continuous monitoring, improvement, and deployment for autonomous driving features","ali nouri, christian berger, fredrik torner","software engineering","using continuous development, deployment, and monitoring (cddm) to understand and improve applications in a customer's context is widely used for non-safety applications such as smartphone apps or web applications to enable rapid and innovative feature improvements. having demonstrated its potential in such domains, it may have the potential to also improve the software development for automotive functions as some oems described on a high level in their financial company communiqus. however, the application of a cddm strategy also faces challenges from a process adherence and documentation perspective as required by safety-related products such as autonomous driving systems (ads) and guided by industry standards such as iso-26262 and iso21448. there are publications on cddm in safety-relevant contexts that focus on safety-critical functions on a rather generic level and thus, not specifically ads or automotive, or that are concentrating only on software and hence, missing out the particular context of an automotive oem: well-established legacy processes and the need of their adaptations, and aspects originating from the role of being a system integrator for software/software, hardware/hardware, and hardware/software. in this paper, particular challenges from the automotive domain to better adopt cddm are identified and discussed to shed light on research gaps to enhance cddm, especially for the software development of safe ads. the challenges are identified from today's industrial well-established ways of working by conducting interviews with domain experts and complemented by a literature study.",2024-03-14
"llm-based agents for automating the enhancement of user story quality: an early report","zheying zhang, maruf rayhan, tomas herda, manuel goisauf, pekka abrahamsson","software engineering","in agile software development, maintaining high-quality user stories is crucial, but also challenging. this study explores the use of large language models to automatically improve the user story quality in austrian post group it agile teams. we developed a reference model for an autonomous llm-based agent system and implemented it at the company. the quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. our findings demonstrate the potential of llms in improving user story quality, contributing to the research on ai role in agile development, and providing a practical example of the transformative impact of ai in an industry setting.",2024-03-14
"qcshqd: quantum computing as a service for hybrid classical-quantum software development: a vision","maryam tavassoli sabzevari, matteo esposito, arif ali khan, davide taibi","software engineering","quantum computing (qc) is transitioning from theoretical frameworks to an indispensable powerhouse of computational capability, resulting in extensive adoption across both industrial and academic domains. qc presents exceptional advantages, including unparalleled processing speed and the potential to solve complex problems beyond the capabilities of classical computers. nevertheless, academic researchers and industry practitioners encounter various challenges in harnessing the benefits of this technology. the limited accessibility of qc resources for classical developers, and a general lack of domain knowledge and expertise, represent insurmountable barrier, hence to address these challenges, we introduce a framework- quantum computing as a service for hybrid classical-quantum software development (qcshqd), which leverages service-oriented strategies. our framework comprises three principal components: an integrated development environment (ide) for user interaction, an abstraction layer dedicated to orchestrating quantum services, and a service provider responsible for executing services on quantum computer. this study presents a blueprint for qcshqd, designed to democratize access to qc resources for classical developers who want to seamless harness qc power. the vision of qcshqd paves the way for groundbreaking innovations by addressing key challenges of hybridization between classical and quantum computers.",2024-03-13
"devbench: a comprehensive benchmark for software development","bowen li, wenhan wu, ziwei tang, lin shi, john yang, jinyang li, shunyu yao, chen qian, binyuan hui, qicheng zhang, zhiyin yu, he du, ping yang, dahua lin, chao peng, kai chen","computation and language","recent advancements in large language models (llms) have significantly enhanced their coding capabilities. however, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. to this end, we propose devbench, a comprehensive benchmark that evaluates llms across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. devbench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. empirical studies show that current llms, including gpt-4-turbo, fail to solve the challenges presented within devbench. analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. our findings offer actionable insights for the future development of llms toward real-world programming applications. our benchmark is available at this https url",2024-03-13
"software vulnerability and functionality assessment using llms","rasmus ingemann tuffveson jensen, vali tawosi, salwa alamir","software engineering","while code review is central to the software development process, it can be tedious and expensive to carry out. in this paper, we investigate whether and how large language models (llms) can aid with code reviews. our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. to test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. as data, we employ seminal code generation datasets (humaneval and mbpp) along with expert-written code snippets with security vulnerabilities from the common weakness enumeration (cwe). our experiments consider a mixture of three proprietary models from openai and smaller open-source llms. we find that the former outperforms the latter by a large margin. motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. results show that 36.7% of llm-generated descriptions can be associated with true cwe vulnerabilities.",2024-03-13
"donapi: malicious npm packages detector using behavior sequence knowledge mapping","cheng huang (1), nannan wang (1), ziyan wang (1), siqi sun (1), lingzi li (1), junren chen (1), qianchong zhao (1), jiaxuan han (1), zhen yang (1), lei shi (2) ((1) sichuan university, (2) huawei technologies)","cryptography and security","with the growing popularity of modularity in software development comes the rise of package managers and language ecosystems. among them, npm stands out as the most extensive package manager, hosting more than 2 million third-party open-source packages that greatly simplify the process of building code. however, this openness also brings security risks, as evidenced by numerous package poisoning incidents.
in this paper, we synchronize a local package cache containing more than 3.4 million packages in near real-time to give us access to more package code details. further, we perform manual inspection and api call sequence analysis on packages collected from public datasets and security reports to build a hierarchical classification framework and behavioral knowledge base covering different sensitive behaviors. in addition, we propose the donapi, an automatic malicious npm packages detector that combines static and dynamic analysis. it makes preliminary judgments on the degree of maliciousness of packages by code reconstruction techniques and static analysis, extracts dynamic api call sequences to confirm and identify obfuscated content that static analysis can not handle alone, and finally tags malicious software packages based on the constructed behavior knowledge base. to date, we have identified and manually confirmed 325 malicious samples and discovered 2 unusual api calls and 246 api call sequences that have not appeared in known samples.",2024-03-13
"autodev: automated ai-driven development","michele tufano, anisha agarwal, jinu jang, roshanak zilouchian moghaddam, neel sundaresan","software engineering","the landscape of software development has witnessed a paradigm shift with the advent of ai-powered assistants, exemplified by github copilot. however, existing solutions are not leveraging all the potential capabilities available in an ide such as building, testing, executing code, git operations, etc. therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. to fill this gap, we present autodev, a fully automated ai-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. autodev enables users to define complex software engineering objectives, which are assigned to autodev's autonomous ai agents to achieve. these ai agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. they also have access to files, compiler output, build and testing logs, static analysis tools, and more. this enables the ai agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. furthermore, autodev establishes a secure development environment by confining all operations within docker containers. this framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within autodev. in our evaluation, we tested autodev on the humaneval dataset, obtaining promising results with 91.5% and 87.8% of pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment.",2024-03-13
"assessing the influence of toxic and gender discriminatory communication on perceptible diversity in oss projects","sayma sultana, gias uddin, amiangshu bosu","software engineering","the presence of toxic and gender-identity derogatory language in open-source software (oss) communities has recently become a focal point for researchers. such comments not only lead to frustration and disengagement among developers but may also influence their leave from the oss projects. despite ample evidence suggesting that diverse teams enhance productivity, the existence of toxic or gender identity discriminatory communications poses a significant threat to the participation of individuals from marginalized groups and, as such, may act as a barrier to fostering diversity and inclusion in oss projects. however, there is a notable lack of research dedicated to exploring the association between gender-based toxic and derogatory language with a perceptible diversity of open-source software teams. consequently, this study aims to investigate how such content influences the gender, ethnicity, and tenure diversity of open-source software development teams. to achieve this, we extract data from active github projects, assess various project characteristics, and identify instances of toxic and gender-discriminatory language within issue/pull request comments. using these attributes, we construct a regression model to explore how they associate with the perceptible diversity of those projects.",2024-03-12
"lessons from a pioneering software engineering environment: design principles of software through pictures","anthony i. (tony)wasserman","software engineering","this paper describes the historical background that led to the development of the innovative software through pictures multi-user development environment, and the principles for its integration with other software products to create a software engineering environment covering multiple tasks in the software development lifecycle.",2024-03-12
"satdaug -- a balanced and augmented dataset for detecting self-admitted technical debt","edi sutoyo, andrea capiluppi","software engineering","self-admitted technical debt (satd) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. these datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify satd instances. however, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of satd. in order to address the scarcity of labeled data for satd \textit{identification} (i.e., whether an instance is satd or not) and \textit{categorization} (i.e., which type of satd is being classified) in existing datasets, we share the \textit{satdaug} dataset, an augmented version of existing satd datasets, including source code comments, issue tracker, pull requests, and commit messages. these augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models.",2024-03-12
"comparison of static analysis architecture recovery tools for microservice applications","simon schneider, alexander bakhtin, xiaozhou li, jacopo soldani, antonio brogi, tomas cerny, riccardo scandariato, davide taibi","software engineering","architecture recovery tools help software engineers obtain an overview of their software systems during all phases of the software development lifecycle. this is especially important for microservice applications because their distributed nature makes it more challenging to oversee the architecture. various tools and techniques for this task are presented in academic and grey literature sources. practitioners and researchers can benefit from a comprehensive overview of these tools and their abilities. however, no such overview exists that is based on executing the identified tools and assessing their outputs regarding effectiveness. with the study described in this paper, we plan to first identify static analysis architecture recovery tools for microservice applications via a multi-vocal literature review, and then execute them on a common dataset and compare the measured effectiveness in architecture recovery. we will focus on static approaches because they are also suitable for integration into fast-paced ci/cd pipelines.",2024-03-11
"a tool for automated reasoning about traces based on configurable formal semantics","ferhat erata, arda goknil, bedir tekinerdogan, geylani kardas","software engineering","we present tarski, a tool for specifying configurable trace semantics to facilitate automated reasoning about traces. software development projects require that various types of traces be modeled between and within development artifacts. for any given artifact (e.g., requirements, architecture models and source code), tarski allows the user to specify new trace types and their configurable semantics, while, using the semantics, it automatically infers new traces based on existing traces provided by the user, and checks the consistency of traces. it has been evaluated on three industrial case studies in the automotive domain (this https url).",2024-03-09
"legion: harnessing pre-trained language models for github topic recommendations with distribution-balance loss","yen-trang dang, thanh-le cong, phuc-thanh nguyen, anh m. t. bui, phuong t. nguyen, bach le, quyet-thang huynh","software engineering","open-source development has revolutionized the software industry by promoting collaboration, transparency, and community-driven innovation. today, a vast amount of various kinds of open-source software, which form networks of repositories, is often hosted on github - a popular software development platform. to enhance the discoverability of the repository networks, i.e., groups of similar repositories, github introduced repository topics in 2017 that enable users to more easily explore relevant projects by type, technology, and more. it is thus crucial to accurately assign topics for each github repository. current methods for automatic topic recommendation rely heavily on tf-idf for encoding textual data, presenting challenges in understanding semantic nuances. this paper addresses the limitations of existing techniques by proposing legion, a novel approach that leverages pre-trained language models (ptms) for recommending topics for github repositories. the key novelty of legion is three-fold. first, legion leverages the extensive capabilities of ptms in language understanding to capture contextual information and semantic meaning in github repositories. second, legion overcomes the challenge of long-tailed distribution, which results in a bias toward popular topics in ptms, by proposing a distribution-balanced loss (db loss) to better train the ptms. third, legion employs a filter to eliminate vague recommendations, thereby improving the precision of ptms. our empirical evaluation on a benchmark dataset of real-world github repositories shows that legion can improve vanilla ptms by up to 26% on recommending githubs topics. legion also can suggest github topics more precisely and effectively than the state-of-the-art baseline with an average improvement of 20% and 5% in terms of precision and f1-score, respectively.",2024-03-09
"mining issue trackers: concepts and techniques","lloyd montgomery, clara lüders, walid maalej","software engineering","an issue tracker is a software tool used by organisations to interact with users and manage various aspects of the software development lifecycle. with the rise of agile methodologies, issue trackers have become popular in open and closed-source settings alike. internal and external stakeholders report, manage, and discuss ""issues"", which represent different information such as requirements and maintenance tasks. issue trackers can quickly become complex ecosystems, with dozens of projects, hundreds of users, thousands of issues, and often millions of issue evolutions. finding and understanding the relevant issues for the task at hand and keeping an overview becomes difficult with time. moreover, managing issue workflows for diverse projects becomes more difficult as organisations grow, and more stakeholders get involved. to help address these difficulties, software and requirements engineering research have suggested automated techniques based on mining issue tracking data. given the vast amount of textual data in issue trackers, many of these techniques leverage natural language processing. this chapter discusses four major use cases for algorithmically analysing issue data to assist stakeholders with the complexity and heterogeneity of information in issue trackers. the chapter is accompanied by a follow-along demonstration package with jupyternotebooks.",2024-03-08
"commitbench: a benchmark for commit message generation","maximilian schall, tamara czinczoll, gerard de melo","computation and language","writing commit messages is a tedious daily task for many software developers, and often remains neglected. automating this task has the potential to save time while ensuring that messages are informative. a high-quality dataset and an objective benchmark are vital preconditions for solid research and evaluation towards this goal. we show that existing datasets exhibit various problems, such as the quality of the commit selection, small sample sizes, duplicates, privacy issues, and missing licenses for redistribution. this can lead to unusable models and skewed evaluations, where inferior models achieve higher evaluation scores due to biases in the data. we compile a new large-scale dataset, commitbench, adopting best practices for dataset creation. we sample commits from diverse projects with licenses that permit redistribution and apply our filtering and dataset enhancements to improve the quality of generated commit messages. we use commitbench to compare existing models and show that other approaches are outperformed by a transformer model pretrained on source code. we hope to accelerate future research by publishing the source code( this https url ).",2024-03-08
"effective fault localization using probabilistic and grouping approach","saksham sahai srivastava, arpita dutta, rajib mall","software engineering","context: fault localization (fl) is the key activity while debugging a program. any improvement to this activity leads to significant improvement in total software development cost. there is an internal linkage between the program spectrum and test execution result. conditional probability in statistics captures the probability of occurring one event in relationship to one or more other events. objectives: the aim of this paper is to use the conception of conditional probability to design an effective fault localization technique. methods: in the paper, we present a fault localization technique that derives the association between statement coverage information and test case execution result using condition probability statistics. this association with the failed test case result shows the fault containing the probability of that specific statement. subsequently, we use a grouping method to refine the obtained statement ranking sequence for better fault localization. results: we evaluated the effectiveness of proposed method over eleven open-source data sets. our obtained results show that on average, the proposed cgfl method is 24.56% more effective than other contemporary fault localization methods such as d*, tarantula, ochiai, crosstab, bpnn, rbfnn, dnn, and cnn. conclusion: we devised an effective fault localization technique by combining the conditional probabilistic method with failed test case execution-based approach. our experimental evaluation shows our proposed method outperforms the existing fault localization techniques.",2024-03-08
"exploring llm-based agents for root cause analysis","devjeet roy, xuchao zhang, rashi bhave, chetan bansal, pedro las-casas, rodrigo fonseca, saravan rajmohan","software engineering","the growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. root cause analysis (rca), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. automation of rca can result in significant savings of time, and ease the burden of incident management on on-call engineers. recently, researchers have utilized large language models (llms) to perform rca, and have demonstrated promising results. however, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. in this work, we explore the use of llm based agents for rca to address this limitation. we present a thorough empirical evaluation of a react agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at microsoft. results show that react performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. we then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. lastly, we conduct a case study with a team at microsoft to equip the react agent with tools that give it access to external diagnostic services that are used by the team for manual rca. our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.",2024-03-07
"learn to code sustainably: an empirical study on llm-based green code generation","tina vartziotis, ippolyti dellatolas, george dasoulas, maximilian schmidt, florian schneider, tim hoffmann, sotirios kotsopoulos, michael keckeisen","software engineering","the increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. these contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (ai) models. the need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of ai models can lead to energy efficiency gains. here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of ai models. in this framework, we evaluate the sustainability of auto-generated code. the auto-generate codes considered in this study are produced by generative commercial ai language models, github copilot, openai chatgpt-3, and amazon codewhisperer. within our methodology, in order to quantify the sustainability awareness of these ai models, we propose a definition of the code's ""green capacity"", based on certain sustainability metrics. we compare the performance and green capacity of human-generated code and code generated by the three ai language models in response to easy-to-hard problem statements. our findings shed light on the current capacity of ai models to contribute to sustainable software development.",2024-03-05
"alloyinecore: embedding of first-order relational logic into meta-object facility for automated model reasoning","ferhat erata, arda goknil, ivan kurtev, bedir tekinerdogan","software engineering","we present alloyinecore, a tool for specifying metamodels with their static semantics to facilitate automated, formal reasoning on models. software development projects require that software systems be specified in various models (e.g., requirements models, architecture models, test models, and source code). it is crucial to reason about those models to ensure the correct and complete system specifications. alloyinecore allows the user to specify metamodels with their static semantics, while, using the semantics, it automatically detects inconsistent models, and completes partial models. it has been evaluated on three industrial case studies in the automotive domain (this https url).",2024-03-05
"can llms generate architectural design decisions? -an exploratory empirical study","rudra dhar, karthik vaidhyanathan, vasudeva varma","software engineering","architectural knowledge management (akm) involves the organized handling of information related to architectural decisions and design within a project or organization. an essential artifact of akm is the architecture decision records (adr), which documents key design decisions. adrs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. despite their benefits, adr adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. recent advancements in large language models (llms) may help bridge this adoption gap by facilitating adr generation. however, the effectiveness of llm for adr generation or understanding is something that has not been explored. to this end, in this work, we perform an exploratory study that aims to investigate the feasibility of using llm for the generation of adrs given the decision context. in our exploratory study, we utilize gpt and t5-based models with 0-shot, few-shot, and fine-tuning approaches to generate the decision of an adr given its context. our results indicate that in a 0-shot setting, state-of-the-art models such as gpt-4 generate relevant and accurate design decisions, although they fall short of human-level performance. additionally, we observe that more cost-effective models like gpt-3.5 can achieve similar outcomes in a few-shot setting, and smaller models such as flan-t5 can yield comparable results after fine-tuning. to conclude, this exploratory study suggests that llm can generate design decisions, but further research is required to attain human-level generation and establish standardized widespread adoption.",2024-03-04
"a systematic evaluation of large language models for generating programming code","wenpin hou, zhicheng ji","software engineering","we systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. gpt-4 substantially outperforms other large language models, including gemini ultra and claude 2. the coding performance of gpt-4 varies considerably with different prompt strategies. in most leetcode and geeksforgeeks coding contests evaluated in this study, gpt-4 employing the optimal prompt strategy outperforms 85 percent of human participants. additionally, gpt-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. the computational efficiency of the code generated by gpt-4 is comparable to that of human programmers. these results suggest that gpt-4 has the potential to serve as a reliable assistant in programming code generation and software development.",2024-03-01
"reusable mlops: reusable deployment, reusable infrastructure and hot-swappable machine learning models and services","d panchal, p verma, i baran, d musgrove, d lu","software engineering","although machine learning model building has become increasingly accessible due to a plethora of tools, libraries and algorithms being available freely, easy operationalization of these models is still a problem. it requires considerable expertise in data engineering, software development, cloud and devops. it also requires planning, agreement, and vision of how the model is going to be used by the business applications once it is in production, how it is going to be continuously trained on fresh incoming data, and how and when a newer model would replace an existing model. this leads to developers and data scientists working in silos and making suboptimal decisions. it also leads to wasted time and effort. we introduce the acumos ai platform we developed and we demonstrate some unique novel capabilities that the acumos model runner possesses, that can help solve the above problems. we introduce a new sustainable concept in the field of ai/ml operations - called reusable mlops - where we reuse the existing deployment and infrastructure to serve new models by hot-swapping them without tearing down the infrastructure or the microservice, thus achieving reusable deployment and operations for ai/ml models while still having continuously trained models in production.",2024-02-19
"are your comments outdated? towards automatically detecting code-comment consistency","yuan huang, yinan chen, xiangping chen, xiaocong zhou","software engineering","in software development and maintenance, code comments can help developers understand source code, and improve communication among developers. however, developers sometimes neglect to update the corresponding comment when changing the code, resulting in outdated comments (i.e., inconsistent codes and comments). outdated comments are dangerous and harmful and may mislead subsequent developers. more seriously, the outdated comments may lead to a fatal flaw sometime in the future. to automatically identify the outdated comments in source code, we proposed a learning-based method, called cocc, to detect the consistency between code and comment. to efficiently identify outdated comments, we extract multiple features from both codes and comments before and after they change. besides, we also consider the relation between code and comment in our model. experiment results show that cocc can effectively detect outdated comments with precision over 90%. in addition, we have identified the 15 most important factors that cause outdated comments, and verified the applicability of cocc in different programming languages. we also used cocc to find outdated comments in the latest commits of open source projects, which further proves the effectiveness of the proposed method.",2024-03-01
"a naive approach for automatic line-level code completion","shamima naznin, dr.manishankar mondal","software engineering","coding is an integral aspect of programming. a programmer can automatically complete a code fragment after writing a few tokens, and the process of automatic completion is known as code completion. several research studies on code completion have previously been conducted for method body completion and method parameter completion. however, this fundamental study explores the automatic completion of any program statement that might not even be part of a method.
the goal is to provide suggestions to the programmer for completing code throughout the codebase by identifying and analyzing code similarities. the proposed methodology can be regarded as a fundamental framework for automated code completion. from the investigation of hundreds of revisions of four subject systems written in c and java, it is observed that the proposed method can automatically complete around 22% of code statements with an average accuracy of 87% that a programmer writes during development, accelerating software development time. the empirical analysis further demonstrates that the approach can be used with programming language neutrality.
the study concludes by illustrating that taking 10 characters as prefixes before invoking completion provides maximum precision.",2024-02-29
"devphish: exploring social engineering in software supply chain attacks on developers","hossein siadati, sima jafarikhah, elif sahin, terrence brent hernandez, elijah lorenzo tripp, denis khryashchev","software engineering","the software supply chain (ssc) has captured considerable attention from attackers seeking to infiltrate systems and undermine organizations. there is evidence indicating that adversaries utilize social engineering (soce) techniques specifically aimed at software developers. that is, they interact with developers at critical steps in the software development life cycle (sdlc), such as accessing github repositories, incorporating code dependencies, and obtaining approval for pull requests (pr) to introduce malicious code. this paper aims to comprehensively explore the existing and emerging soce tactics employed by adversaries to trick software engineers (swes) into delivering malicious software. by analyzing a diverse range of resources, which encompass established academic literature and real-world incidents, the paper systematically presents an overview of these manipulative strategies within the realm of the ssc. such insights prove highly beneficial for threat modeling and security gap analysis.",2024-02-28
"low-modeling of software systems","jordi cabot","software engineering","there is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems. new types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle. in the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage. in this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems.",2024-02-28
"autonomous vehicles: evolution of artificial intelligence and learning algorithms","divya garikapati, sneha sudhir shetiya","machine learning","the advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. central to this evolution is the integration of artificial intelligence (ai) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. this paper provides a comprehensive exploration of the evolutionary trajectory of ai within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. commencing with a current landscape overview, the paper delves into the fundamental role of ai in shaping the autonomous decision-making capabilities of vehicles. it elucidates the steps involved in the ai-powered development life cycle in vehicles, addressing ethical considerations and bias in ai-driven software development for autonomous vehicles. the study presents statistical insights into the usage and types of ai/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry. furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. it concludes by outlining different levels of autonomy, elucidating the nuanced usage of ai and learning algorithms, and automating key tasks at each level. additionally, the document discusses the variation in software package sizes across different autonomy levels",2024-02-27
"chronicles of ci/cd: a deep dive into its usage over time","hugo da gião, andré flores, rui pereira, jácome cunha","software engineering","devops is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality. part of this process is ci/cd, which embodies mostly the first parts, right up to the deployment. despite the many benefits of devops and ci/cd, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date. software repositories contain data regarding various software practices, tools, and uses. this data can help gather multiple insights that inform technical and academic decision-making. github is currently the most popular software hosting platform and provides a search api that lets users query its repositories. our goal with this paper is to gain insights into the technologies developers use for ci/cd by analyzing github repositories. using a list of the state-of-the-art ci/cd technologies, we use the github search api to find repositories using each of these technologies. we also use the api to extract various insights regarding those repositories. we then organize and analyze the data collected. from our analysis, we provide an overview of the use of ci/cd technologies in our days, but also what happened in the last 12 years. we also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common. from these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their ci/cd pipelines, again considering the various dimensions of the problem.",2024-02-27
"leveraging power of deep learning for fast and efficient elite pixel selection in time series sar interferometry","ashutosh tiwari, nitheshnirmal sadhashivam, leonard o. ohenhen, manoochehr shirzaei","signal processing","this work proposes an improved convolutional long short-term memory (convlstm) based architecture for selection of elite pixels (i.e., less noisy) in time series interferometric synthetic aperture radar (ts-insar). compared to previous version, the model can process insar stacks of variable time steps and select both persistent (ps) and distributed scatterers (ds). we trained the model on ~20,000 training images (interferograms), each of size 100 by 100 pixels, extracted from insar time series interferograms containing both artificial features (buildings and infrastructure) and objects of natural environment (vegetation, forests, barren or agricultural land, water bodies). based on such categorization, we developed two deep learning models, primarily focusing on urban and coastal sites. training labels were generated from elite pixel selection outputs generated from the wavelet-based insar (wabinsar) software developed by shirzaei (2013) and improved in lee and shirzaei (2023). with 4 urban and 7 coastal sites used for training and validation, the predicted elite pixel selection maps reveal that the proposed models efficiently learn from wabinsar-generated labels, reaching a validation accuracy of 94%. the models accurately discard pixels affected by geometric and temporal decorrelation while selecting pixels corresponding to urban objects and those with stable phase history unaffected by temporal and geometric decorrelation. the density of pixels in urban areas is comparable to and higher for coastal areas compared to wabinsar outputs. with significantly reduced time computation (order of minutes) and improved selection of elite pixels, the proposed models can efficiently process long insar time series stacks and generate rapid deformation maps.",2024-02-26
"rethinking software engineering in the foundation model era: a curated catalogue of challenges in the development of trustworthy fmware","ahmed e. hassan, dayi lin, gopi krishnan rajbahadur, keheliya gallaba, filipe r. cogo, boyuan chen, haoxiang zhang, kishanthan thangarajah, gustavo ansaldi oliva, jiahuei lin, wali mohammad abdullah, zhen ming jiang","software engineering","foundation models (fms), such as large language models (llms), have revolutionized software development by enabling new use cases and business models. we refer to software built using fms as fmware. the unique properties of fmware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of fms (e.g., hallucination) lead to a completely new set of software engineering challenges. based on our industrial experience, we identified 10 key se4fmware challenges that have caused enterprise fmware development to be unproductive, costly, and risky. in this paper, we discuss these challenges in detail and state the path for innovation that we envision. next, we present fmarts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy fmware. finally, we (i) show how the unique properties of fmarts enabled us to design and develop a complex fmware for a large customer in a timely manner and (ii) discuss the lessons that we learned in doing so. we hope that the disclosure of the aforementioned challenges and our associated efforts to tackle them will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions across the software engineering discipline.",2024-02-25
"chain-of-specificity: an iteratively refining method for eliciting knowledge from large language models","kaiwen wei, jingyuan zhang, hongzhi zhang, fuzheng zhang, di zhang, li jin, yue yu","artificial intelligence","large language models (llms) exhibit remarkable generative capabilities, enabling the generation of valuable information. despite these advancements, previous research found that llms sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory. existing approaches attempted to address this issue by decomposing or rewriting input instructions, yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g., programming within the context of software development). in response, this paper proposes a simple yet effective method named chain-of-specificity (cos). specifically, cos iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within llms, and refines responses. experiments conducted on publicly available and self-build complex datasets demonstrate that cos outperforms existing methods in enhancing generated content especially for the specificity. besides, as the number of specific constraints increase, other baselines falter, while cos still performs well. moreover, we show that distilling responses generated by cos effectively enhances the ability of smaller models to follow the constrained instructions. resources of this paper will be released for further research.",2024-02-20
"studying llm performance on closed- and open-source data","toufique ahmed, christian bird, premkumar devanbu, saikat chakraborty","software engineering","large language models (llms) are finding wide use in software engineering practice. these models are extremely data-hungry, and are largely trained on open-source (oss) code distributed with permissive licenses. in terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use llms, in settings where the models may not be as familiar with the code under development. in such settings, do llms work as well as they do for oss code? if not, what are the differences? when performance differs, what are the possible causes, and are there work-arounds? in this paper, we examine this issue using proprietary, closed-source software data from microsoft, where most proprietary code is in c# and c++. we find that performance for c# changes little from oss --> proprietary code, but does significantly reduce for c++; we find that this difference is attributable to differences in identifiers. we also find that some performance degradation, in some cases, can be ameliorated efficiently by in-context learning.",2024-02-23
"llm-compdroid: repairing configuration compatibility bugs in android apps with pre-trained large language models","zhijie liu, yutian tang, meiyun li, xin jin, yunfei long, liang feng zhang, xiapu luo","software engineering","xml configurations are integral to the android development framework, particularly in the realm of ui display. however, these configurations can introduce compatibility issues (bugs), resulting in divergent visual outcomes and system crashes across various android api versions (levels). in this study, we systematically investigate llm-based approaches for detecting and repairing configuration compatibility bugs. our findings highlight certain limitations of llms in effectively identifying and resolving these bugs, while also revealing their potential in addressing complex, hard-to-repair issues that traditional tools struggle with. leveraging these insights, we introduce the llm-compdroid framework, which combines the strengths of llms and traditional tools for bug resolution. our experimental results demonstrate a significant enhancement in bug resolution performance by llm-compdroid, with llm-compdroid-gpt-3.5 and llm-compdroid-gpt-4 surpassing the state-of-the-art tool, conffix, by at least 9.8% and 10.4% in both correct and correct@k metrics, respectively. this innovative approach holds promise for advancing the reliability and robustness of android applications, making a valuable contribution to the field of software development.",2024-02-23
"agile requirement change management model for global software development","neha koulecar, bachan ghimire","software engineering","we propose a noble, comprehensive and robust agile requirements change management (arcm) model that addresses the limitations of existing models and is tailored for agile software development in the global software development paradigm. to achieve this goal, we conducted an exhaustive literature review and an empirical study with rcm industry experts. our study evaluated the effectiveness of the proposed rcm model in a real-world setting and identifies any limitations or areas for improvement. the results of our study provide valuable insights into how the proposed rcm model can be applied in agile global software development environments to improve software development practices and optimize project success rates.",2024-02-22
"bugfix: towards a common language and framework for the automaticprogram repair community","bertrand meyer, viktoryia kananchuk, li huang","software engineering","techniques of automatic program repair (apr) have the potential of thoroughly facilitating the task of producing quality software. after a promising start, however, progress in making apr practical has been hindered by the lack of a common framework to support the multiplicity of apr ideas and tools, and of target programming languages and environments.
in this position paper we outline a general framework to enable the apr community to benefit from each otherś advances, in particular through a standard language for describing bugs and their fixes. such a common framework (which is also applicable to work on fault seeding) could be a tremendous benefit to researchers and developers of interactive development environments (ides) who are working to make apr an effective part of the practical experience of software developers.",2024-02-22
"copilot evaluation harness: evaluating llm-guided software programming","anisha agarwal, aaron chan, shubham chandel, jinu jang, shaun miller, roshanak zilouchian moghaddam, yevhen mohylevskyy, neel sundaresan, michele tufano","software engineering","the integration of large language models (llms) into development environments (ides) has become a focal point in modern software development. llms such as openai gpt-3.5/4 and code llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. however, utilizing llms out of the box is unlikely to be optimal for any given scenario. rather, each system requires the llm to be honed to its set of heuristics to ensure the best performance. in this paper, we introduce the copilot evaluation harness: a set of data and tools for evaluating llm-guided ide interactions, covering various programming scenarios and languages. we propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. we design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, including code generation from natural language (generate), documentation generation from code (doc), test case generation (test), bug-fixing (fix), and workspace understanding and query resolution (workspace). these success metrics are designed to evaluate the performance of llms within a given ide and its respective parameter space. our learnings from evaluating three common llms using these metrics can inform the development and validation of future scenarios in llm guided ides.",2024-02-22
"test-driven development for code generation","noble saji mathews, meiyappan nagappan","software engineering","large language models (llms) like gpt4, have shown proficiency in generating code snippets from problem statements. traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. however, in the past, there have been several studies that have shown the value of test-driven development (tdd) where humans write tests based on problem statements before the code for the functionality is written. in the context of llm-based code generation, one obvious benefit of tdd is that the developer then knows for sure if the generated code has passed all the given tests or not. therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to gpt4 is better than just giving the problem statement as input. to test our hypothesis, we build a framework tgen. in our experiments on the mbpp, humaneval and codechef datasets, we consistently find that including tests solves more programming problems than not including them. thus we show that tdd is a better development model than just using a problem statement when using gpt4 for code generation tasks.",2024-02-21
"a strategic model of software dependency networks","cornelius fritz, co-pierre georg, angelo mele, michael schweinberger","econometrics","modern software development involves collaborative efforts and reuse of existing code, which reduces the cost of developing new software. however, reusing code from existing packages exposes coders to vulnerabilities in these dependencies. we study the formation of dependency networks among software packages and libraries, guided by a structural model of network formation with observable and unobservable heterogeneity. we estimate costs, benefits, and link externalities of the network of 696,790 directed dependencies between 35,473 repositories of the rust programming language using a novel scalable algorithm. we find evidence of a positive externality exerted on other coders when coders create dependencies. furthermore, we show that coders are likely to link to more popular packages of the same software type but less popular packages of other types. we adopt models for the spread of infectious diseases to measure a package's systemicness as the number of downstream packages a vulnerability would affect. systemicness is highly skewed with the most systemic repository affecting almost 90% of all repositories only two steps away. lastly, we show that protecting only the ten most important repositories reduces vulnerability contagion by nearly 40%.",2024-02-20
"a disruptive research playbook for studying disruptive innovations","margaret-anne storey, daniel russo, nicole novielli, takashi kobayashi, dong wang","software engineering","as researchers, we are now witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative ai, augmented reality (ar) and virtual reality (vr). in particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its the socio-technical nature. in this paper, we reflect on the importance of formulating and addressing research in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. we propose a research playbook with the goal of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. we showcase how to apply the research playbook. firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, stack overflow, and its impact on software development. secondly, we show it can be used to question the impact of two current disruptive technologies: ai and ar/vr. finally, we introduce a specialized gpt model to support the researcher in framing future investigations. we conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond.",2024-02-20
"choosing a suitable requirement prioritization method: a survey","esraa alhenawi, shatha awawdeh, ruba abu khurma, maribel garcía-arenas, pedro a. castillo, amjad hudaib","software engineering","software requirements prioritization plays a crucial role in software development. it can be viewed as the process of ordering requirements by determining which requirements must be done first and which can be done later. powerful requirements prioritization techniques are of paramount importance to finish the implementation on time and within budget. many factors affect requirement prioritization such as stakeholder expectations, complexity, dependency, scalability, risk, and cost. therefore, finding the proper order of requirements is a challenging process. hence, different types of requirements prioritization techniques have been developed to support this task. in this survey, we propose a novel classification that can classify the prioritization techniques under two major classes: relative and exact prioritization techniques class, where each class is divided into two subclasses. we depend in our classification on the way the value of ranking is given to the requirement, either explicitly as a specific value in the case of the exact prioritization techniques class, or implicitly in the case of the relative prioritization technique class. an overview of fifteen different requirements prioritization techniques are presented and organized according to the proposed classification criteria's. moreover, we make a comparison between methods that are related to the same subclass to analyze their strengths and weaknesses. based on the comparison results, the properties for each proposed subclass of techniques are identified. depending on these properties, we present some recommendations to help project managers in the process of selecting the most suitable technique to prioritize requirements based on their project characteristics (number of requirements, time, cost, and accuracy).",2024-02-20
"systematic mapping protocol -- ux design role in software development process","emilio ormeño, fernando pinciroli","software engineering","a systematic mapping protocol is a method for conducting a literature review in a rigorous and transparent way. it aims to provide an overview of the current state of research on a specific topic, identify gaps and opportunities, and guide future work. in this document, we present a systematic mapping protocol for investigating the role of the ux designer in the software development process. we define the research questions, scope, sources, search strategy, selection criteria, data extraction, and analysis methods that we will use to conduct the mapping study. our goal is to understand how the ux designers collaborate with other stakeholders, what methods and tools they use, what challenges they face, and what outcomes they achieve in different contexts and domains.",2024-02-20
"inferring non-failure conditions for declarative programs","michael hanus","programming languages","unintended failures during a computation are painful but frequent during software development. failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers. programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct. this paper presents an approach to verify such assumptions. for this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations. in the positive case, the absence of such failures is ensured. in the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again. our method is fully automatic and can be applied to larger declarative programs. the results of an implementation for functional logic curry programs are presented.",2024-02-20
"measuring impacts of poisoning on model parameters and neuron activations: a case study of poisoning codebert","aftab hussain, md rafiqul islam rabin, navid ayoobi, mohammad amin alipour","software engineering","large language models (llms) have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. in this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. specifically, we examine attention weights and biases, activation values, and context embeddings of the clean and poisoned codebert models. our results suggest noticeable patterns in activation values and context embeddings of poisoned samples for the poisoned codebert model; however, attention weights and biases do not show any significant differences. this work contributes to ongoing efforts in white-box detection of backdoor signals in llms of code through the analysis of parameters and activations.",2024-02-20
"challenges and experiences of iranian developers with mlops at enterprise","mohammad heydari, zahra rezvani","software engineering","data is becoming more complex, and so are the approaches designed to process it. enterprises have access to more data than ever, but many still struggle to glean the full potential of insights from what they have. this research explores the challenges and experiences of iranian developers in implementing the mlops paradigm within enterprise settings. mlops, or machine learning operations, is a discipline focused on automating the continuous delivery of machine learning models. in this study, we review the most popular mlops tools used by leading technology enterprises. additionally, we present the results of a questionnaire answered by over 110 iranian machine learning experts and software developers, shedding light on mlops tools and the primary obstacles faced. the findings reveal that data quality problems, a lack of resources, and difficulties in model deployment are among the primary challenges faced by practitioners. collaboration between ml, devops, ops, and science teams is seen as a pivotal challenge in implementing mlops effectively.",2024-02-19
"enhancing large language models for text-to-testcase generation","saranya alagarsamy, chakkrit tantithamthavorn, chetan arora, aldeida aleti","software engineering","context: test-driven development (tdd) is a widely employed software development practice that involves developing test cases based on requirements prior to writing the code. although various methods for automated test case generation have been proposed, they are not specifically tailored for tdd, where requirements instead of code serve as input. objective: in this paper, we introduce a text-to-testcase generation approach based on a large language model (gpt-3.5) that is fine-tuned on our curated dataset with an effective prompt design. method: our approach involves enhancing the capabilities of basic gpt-3.5 for text-to-testcase generation task that is fine-tuned on our curated dataset with an effective prompting design. we evaluated the effectiveness of our approach using a span of five large-scale open-source software projects. results: our approach generated 7k test cases for open source projects, achieving 78.5% syntactic correctness, 67.09% requirement alignment, and 61.7% code coverage, which substantially outperforms all other llms (basic gpt-3.5, bloom, and codet5). in addition, our ablation study demonstrates the substantial performance improvement of the fine-tuning and prompting components of the gpt-3.5 model. conclusions: these findings lead us to conclude that fine-tuning and prompting should be considered in the future when building a language model for the text-to-testcase generation task",2024-02-19
"evaluating program repair with semantic-preserving transformations: a naturalness assessment","thanh le-cong, dat nguyen, bach le, toby murray","software engineering","in this paper, we investigate the naturalness of semantic-preserving transformations and their impacts on the evaluation of npr. to achieve this, we conduct a two-stage human study, including (1) interviews with senior software developers to establish the first concrete criteria for assessing the naturalness of code transformations and (2) a survey involving 10 developers to assess the naturalness of 1178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. our findings reveal that nearly 60% and 20% of these transformations are considered natural and unnatural with substantially high agreement among human annotators. furthermore, the unnatural code transformations introduce a 25.2% false alarm rate on robustness of five well-known npr systems. additionally, the performance of the npr systems drops notably when evaluated using natural transformations, i.e., a drop of up to 22.9% and 23.6% in terms of the numbers of correct and plausible patches generated by these systems. these results highlight the importance of robustness testing by considering naturalness of code transformations, which unveils true effectiveness of npr systems. finally, we conduct an exploration study on automating the assessment of naturalness of code transformations by deriving a new naturalness metric based on cross-entropy. based on our naturalness metric, we can effectively assess naturalness for code transformations automatically with an auc of 0.7.",2024-02-19
"can chatgpt support developers? an empirical evaluation of large language models for code generation","kailun jin, chung-yu wang, hung viet pham, hadi hemmati","software engineering","large language models (llms) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. however, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively llms can support developers in real-world. to address this, we conducted an empirical analysis of conversations in devgpt, a dataset collected from developers' conversations with chatgpt (captured with the share link feature on platforms such as github). our empirical findings indicate that the current practice of using llm-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. these findings indicate that there is much future work needed to improve llms in code generation before they can be integral parts of modern software development.",2024-02-18
"gradients of brain organization: smooth sailing from methods development to user community","jessica royer, casey paquola, sofie l. valk, matthias kirschner, seok-jun hong, bo-yong park, richard a.i. bethlehem, robert leech, b. t. thomas yeo, elizabeth jefferies, jonathan smallwood, daniel margulies, boris c. bernhardt","other quantitative biology","multimodal neuroimaging grants a powerful in vivo window into the structure and function of the human brain. recent methodological and conceptual advances have enabled investigations of the interplay between large-scale spatial trends, or gradients, in brain structure and function, offering a framework to unify principles of brain organization across multiple scales. strong community enthusiasm for these techniques has been instrumental in their widespread adoption and implementation to answer key questions in neuroscience. following a brief review of current literature on this framework, this perspective paper will highlight how pragmatic steps aiming to make gradient methods more accessible to the community propelled these techniques to the forefront of neuroscientific inquiry. more specifically, we will emphasize how interest for gradient methods was catalyzed by data sharing, open-source software development, as well as the organization of dedicated workshops led by a diverse team of early career researchers. to this end, we argue that the growing excitement for brain gradients is the result of coordinated and consistent efforts to build an inclusive community and can serve as a case in point for future innovations and conceptual advances in neuroinformatics. we close this perspective paper by discussing challenges for the continuous refinement of neuroscientific theory, methodological innovation, and real-world translation to maintain our collective progress towards integrated models of brain organization.",2024-02-16
"multidimer: a multi-dimensional bug analyzer","lakmal silva, michael unterkalmsteiner, krzysztof wnuk","software engineering","background: bugs and bug management consumes a significant amount of time and effort from software development organizations. a reduction in bugs can significantly improve the capacity for new feature development. aims: we categorize and visualize dimensions of bug reports to identify accruing technical debt. this evidence can serve practitioners and decision makers not only as an argumentative basis for steering improvement efforts, but also as a starting point for root cause analysis, reducing overall bug inflow. method: we implemented a tool, multidimer, that analyzes and visualizes bug reports. the tool was implemented and evaluated at ericsson. results: we present our preliminary findings using the multidimer for bug analysis, where we successfully identified components generating most of the bugs and bug trends within certain components. conclusions: by analyzing the dimensions provided by multidimer, we show that classifying and visualizing bug reports in different dimensions can stimulate discussions around bug hot spots as well as validating the accuracy of manually entered bug report attributes used in technical debt measurements such as fault slip through.",2024-02-16
"language-driven engineering an interdisciplinary software development paradigm","bernhard steffen, tiziana margaria, alexander bainczyk, steve boßelmann, daniel busch, marc driessen, markus frohme, falk howar, sven jörges, marvin krause, marco krumrey, anna-lena lamprecht, michael lybecait, alnis murtovi, stefan naujokat, johannes neubauer, alexander schieweck, jonas schürmann, steven smyth, barbara steffen, fabian storek, tim tegeler, sebastian teumert, dominic wirkner, philip zweihoff","software engineering","we illustrate how purpose-specific, graphical modeling enables application experts with different levels of expertise to collaboratively design and then produce complex applications using their individual, purpose-specific modeling language. our illustration includes seven graphical integrated modeling environments (imes) that support full code generation, as well as four browser-based applications that were modeled and then fully automatically generated and produced using dime, our most complex graphical ime. while the seven imes were chosen to illustrate the types of languages we support with our language-driven engineering (lde) approach, the four dime products were chosen to give an impression of the power of our lde-generated imes. in fact, equinocs, springer nature's future editorial system for proceedings, is also being fully automatically generated and then deployed at their dordrecht site using a deployment pipeline generated with rig, one of the imes presented. our technology is open source and the products presented are currently in use.",2024-02-16
"penetration testing and legacy systems","sandra smyth","software engineering","as per adusumilli (2015),'70% of corporate business systems today are legacy applications. recent statistics prove that over 60% of it budget is spent on maintaining these legacy systems, showing the rigidity and the fragile nature of these systems.' usually, testing is included during the software development cycle, using testing techniques such as unit testing, integration testing, and system testing before releasing the product. after the software product is released to production, no additional testing is done; the testing process is back to the table only when modifications are made. techniques such as regression testing are included to ensure the changes do not affect existing functionality, but testing nonfunctional features that are rarely included in such regression tests' scope. schrader (2021) affirms that 'legacy systems are often maintained only to ensure function,' and it organizations may fail to consider the cybersecurity perspective to remain secure. legacy systems are a high-risk component for the organization that must be carefully considered when structuring a cyber security strategy. this paper aims to help the reader understand some measures that can be taken to secure legacy systems, explaining what penetration testing is and how this testing technique can help secure legacy systems. keywords: testing, legacy, security, risks, prevention, mitigation, pentesting.",2023-12-17
"reproducing, extending, and analyzing naming experiments","rachel alpern, ido lazer, issar tzachor, hanit hakim, sapir weissbuch, dror g. feitelson","software engineering","naming is very important in software development, as names are often the only vehicle of meaning about what the code is intended to do. a recent study on how developers choose names collected the names given by different developers for the same objects. this enabled a study of these names' diversity and structure, and the construction of a model of how names are created. we reproduce different parts of this study in three independent experiments. importantly, we employ methodological variations rather than striving of an exact replication. when the same results are obtained this then boosts our confidence in their validity by demonstrating that they do not depend on the methodology.
our results indeed corroborate those of the original study in terms of the diversity of names, the low probability of two developers choosing the same name, and the finding that experienced developers tend to use slightly longer names than inexperienced students. we explain name diversity by performing a new analysis of the names, classifying the concepts represented in them as universal (agreed upon), alternative (reflecting divergent views on a topic), or optional (reflecting divergent opinions on whether to include this concept at all). this classification enables new research directions concerning the considerations involved in naming decisions. we also show that explicitly using the model proposed in the original study to guide naming leads to the creation of better names, whereas the simpler approach of just asking participants to use longer and more detailed names does not.",2024-02-15
"practitioners' challenges and perceptions of ci build failure predictions at atlassian","yang hong, chakkrit tantithamthavorn, jirat pasuksmit, patanamon thongtanunam, arik friedman, xing zhao, anton krasikov","software engineering","continuous integration (ci) build failures could significantly impact the software development process and teams, such as delaying the release of new features and reducing developers' productivity. in this work, we report on an empirical study that investigates ci build failures throughout product development at atlassian. our quantitative analysis found that the repository dimension is the key factor influencing ci build failures. in addition, our qualitative survey revealed that atlassian developers perceive ci build failures as challenging issues in practice. furthermore, we found that the ci build prediction can not only provide proactive insight into ci build failures but also facilitate the team's decision-making. our study sheds light on the challenges and expectations involved in integrating ci build prediction tools into the bitbucket environment, providing valuable insights for enhancing ci processes.",2024-02-15
"assessing test artifact quality -- a tertiary study","huynh khanh vi tran, michael unterkalmsteiner, jürgen börstler, nauman bin ali","software engineering","context: modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. this puts high demands on the quality of the central artifacts in software testing, test suites and test cases. objective: we aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives. method: we have carried out a systematic literature review to identify and analyze existing secondary studies on quality aspects of software testing artifacts. results: we identified 49 relevant secondary studies. of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. we present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. we also provide a comprehensive model of test case/suite quality with definitions for the quality attributes and measurements based on findings in the literature and iso/iec 25010:2011. conclusion: the test artifact quality model presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. furtherm information and software technology 139 (2021): 106620ore, the model can also be used as a framework for documenting context characteristics to make research results more accessible for research and practice.",2024-02-14
"trained without my consent: detecting code inclusion in language models trained on code","vahid majdinasab, amin nikanjam, foutse khomh","software engineering","code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. the recent advent of large language models (llms) as coding assistants in the software development process poses new challenges for code auditing. the dataset for training these models is mainly collected from publicly available sources. this raises the issue of intellectual property infringement as developers' codes are already included in the dataset. therefore, auditing code developed using llms is challenging, as it is difficult to reliably assert if an llm used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. to address this challenge, we propose a new approach, trawic; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an llm's training dataset. we extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. in our experiments, we observe that trawic is capable of detecting 83.87% of codes that were used to train an llm. in comparison, the prevalent clone detection tool nicad is only capable of detecting 47.64%. in addition to its remarkable performance, trawic has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like codewhisperer reference tracker, across thousands of code snippets.",2024-02-14
"context composing for full line code completion","anton semenkin, yaroslav sokolov, evgeniia vu","software engineering","code completion is one of the most used integrated development environment (ide) features, which affects the everyday life of a software developer. modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. this change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. at jetbrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. we managed to ship the full line code completion feature to pycharm pro ide and proved its usefulness in a/b testing on hundreds of real python users. the paper describes our approach to context composing for the transformer model that is a core of the feature's implementation. in addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.",2024-02-14
"generative ai for pull request descriptions: adoption, impact, and developer interventions","tao xiao, hideaki hata, christoph treude, kenichi matsumoto","software engineering","github's copilot for pull requests (prs) is a promising service aiming to automate various developer tasks related to prs, such as generating summaries of changes or providing complete walkthroughs with links to the relevant code. as this innovative technology gains traction in the open source software (oss) community, it is crucial to examine its early adoption and its impact on the development process. additionally, it offers a unique opportunity to observe how developers respond when they disagree with the generated content. in our study, we employ a mixed-methods approach, blending quantitative analysis with qualitative insights, to examine 18,256 prs in which parts of the descriptions were crafted by generative ai. our findings indicate that: (1) copilot for prs, though in its infancy, is seeing a marked uptick in adoption. (2) prs enhanced by copilot for prs require less review time and have a higher likelihood of being merged. (3) developers using copilot for prs often complement the automated descriptions with their manual input. these results offer valuable insights into the growing integration of generative ai in software development.",2024-02-14
"taking gpu programming models to task for performance portability","joshua h. davis, pranav sivaraman, joy kitson, konstantinos parasyris, harshitha menon, isaac minn, giorgis georgakoudis, abhinav bhatele","distributed, parallel, and cluster computing","ensuring high productivity in scientific software development necessitates developing and maintaining a single codebase that can run efficiently on a range of accelerator-based supercomputing platforms. while prior work has investigated the performance portability of a few selected proxy applications or programming models, this paper provides a comprehensive study of a range of proxy applications implemented in the major programming models suitable for gpu-based platforms. we present and analyze performance results across nvidia and amd gpu hardware currently deployed in leadership-class computing facilities using a representative range of scientific codes and several programming models -- cuda, hip, kokkos, raja, openmp, openacc, and sycl. based on the specific characteristics of applications tested, we include recommendations to developers on how to choose the right programming model for their code. we find that kokkos, raja, and sycl in particular offer the most promise empirically as performance portable programming models. these results provide a comprehensive evaluation of the extent to which each programming model for heterogeneous systems provides true performance portability in real-world usage.",2024-02-14
"chatgpt vs llama: impact, reliability, and challenges in stack overflow discussions","leuson da silva, jordan samhi, foutse khomh","software engineering","since its release in november 2022, chatgpt has shaken up stack overflow, the premier platform for developers' queries on programming and software development. demonstrating an ability to generate instant, human-like responses to technical questions, chatgpt has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative ai. two months after chatgpt's release, meta released its answer with its own large language model (llm) called llama: the race was on. we conducted an empirical study analyzing questions from stack overflow and using these llms to address them. this way, we aim to (ii) measure user engagement evolution with stack overflow over time; (ii) quantify the reliability of llms' answers and their potential to replace stack overflow in the long term; (iii) identify and understand why llms fails; and (iv) compare llms together. our empirical results are unequivocal: chatgpt and llama challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. furthermore, we also discuss the impact of our findings regarding the usage and development of new llms.",2024-02-13
"on-the-fly syntax highlighting: generalisation and speed-ups","marco edoardo palma, alex wolf, pasquale salza, harald c. gall","software engineering","on-the-fly syntax highlighting is the task of rapidly associating visual secondary notation values with each character of a language derivation. research in this domain is driven by the prevalence of online software development tools, which frequently display source code on screen and heavily rely on syntax highlighting mechanisms. in this context, three contrasting demands confront resolvers in this space: speed, accuracy, and development costs. speed constraints are essential to ensure tool usability, manifesting as responsiveness for end users accessing online source code and minimising system overhead. simultaneously, achieving precise highlighting is critical for enhancing code comprehensibility. nevertheless, obtaining accurate results necessitates the capacity to perform grammatical analysis on the code under consideration, even in cases of varying grammatical correctness. furthermore, addressing the development costs of such resolvers is imperative, given the multitude of programming language versions. the current state-of-the-art approach in this field leverages the original lexer and parser of programming languages to create syntax highlighting oracles, subsequently used for training base recurrent neural network models. as the question of the generalisation of such a solution persists, this paper addresses this aspect by extending the original work to three additional mainstream programming languages and conducting a comprehensive review of the outcomes. moreover, the original limitations in evaluation performance and training costs are mitigated through the introduction of a novel convolutional based neural network model. this study examines the performance gains of running models on gpus, finding that the new cnn implementation is much faster than previous methods while maintaining high accuracy.",2024-02-13
"the current state of security -- insights from the german software industry","timo langstrof, alex r. sabau","cryptography and security","these days, software development and security go hand in hand. numerous techniques and strategies are discussed in the literature that can be applied to guarantee the incorporation of security into the software development process. in this paper the main ideas of secure software development that have been discussed in the literature are outlined. next, a dataset on implementation in practice is gathered through a qualitative interview research involving 20 companies. trends and correlations in this dataset are found and contrasted with theoretical ideas from the literature. the results show that the organizations that were polled are placing an increasing focus on security. although the techniques covered in the literature are being used in the real world, they are frequently not fully integrated into formal, standardized processes. the insights gained from our research lay the groundwork for future research, which can delve deeper into specific elements of these methods to enhance our understanding of their application in real-world scenarios.",2024-02-13
"what the fix? a study of asats rule documentation","corentin latappy, thomas degueule, jean-rémy falleri (labri), romain robbes (cnrs, labri, ub, bordeaux inp), xavier blanc, cédric teyton","software engineering","automatic static analysis tools (asats) are widely used by software developers to diffuse and enforce coding practices. yet, we know little about the documentation of asats, despite it being critical to learn about the coding practices in the first place. we shed light on this through several contributions. first, we analyze the documentation of more than 100 rules of 16 asats for multiple programming languages, and distill a taxonomy of the purposes of the documentation-what triggers a rule; why it is important; and how to fix an issue-and its types of contents. then, we conduct a survey to assess the effectiveness of the documentation in terms of its goals and types of content. we highlight opportunities for improvement in asat documentation. in particular, we find that the why purpose is missing in half of the rules we survey; moreover, when the why is present, it is more likely to have quality issues than the what and the fix.",2024-02-13
"turtlerabbit 2024 ssl team description paper","linh trinh, alif anzuman, eric batkhuu, dychen chan, lisa graf, darpan gurung, tharunimm jamal, jigme namgyal, jason ng, wing lam tsang, x. rosalind wang, eren yilmaz, oliver obst","robotics","turtlerabbit is a new robocup ssl team from western sydney university. this team description paper presents our approach in navigating some of the challenges in developing a new ssl team from scratch. ssl is dominated by teams with extensive experience and customised equipment that has been developed over many years. here, we outline our approach in overcoming some of the complexities associated with replicating advanced open-sourced designs and managing the high costs of custom components. opting for simplicity and cost-effectiveness, our strategy primarily employs off-the-shelf electronics components and ``hobby'' brushless direct current (bldc) motors, complemented by 3d printing and cnc milling. this approach helped us to streamline the development process and, with our open-sourced hardware design, hopefully will also lower the bar for other teams to enter robocup ssl in the future. the paper details the specific hardware choices, their approximate costs, the integration of electronics and mechanics, and the initial steps taken in software development, for our entry into ssl that aims to be simple yet competitive.",2024-02-13
"mercury: an efficiency benchmark for llm code synthesis","mingzhe du, anh tuan luu, bin ji, see-kiong ng","software engineering","despite advancements in evaluating large language models (llms) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. we present mercury, the first benchmark designated for assessing the code efficiency of llm code synthesis tasks. mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. unlike existing benchmarks, mercury integrates a novel metric beyond@k to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. our findings reveal that while llms demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for llm research and development.",2024-02-12
"best practices for facing the security challenges of internet of things devices focusing on software development life cycle","md rafid islam, ratun rahman","software engineering","in the past few years, the number of iot devices has grown substantially, and this trend is likely to continue. an increasing amount of effort is being put into developing software for the ever-increasing iot devices. every iot system at its core has software that enables the devices to function efficiently. but security has always been a concern in this age of information and technology. security for iot devices is now a top priority due to the growing number of threats. this study introduces best practices for ensuring security in the iot, with an emphasis on guidelines to be utilized in software development for iot devices. the objective of the study is to raise awareness of potential threats, emphasizing the secure software development lifecycle. the study will also serve as a point of reference for future developments and provide a solid foundation for securing iot software and dealing with vulnerabilities.",2024-02-12
